<!doctype html><html xmlns="http://www.w3.org/1999/xhtml" xmlns:o="urn:schemas-microsoft-com:office:office"><head><!--[if gte mso 9]><xml><o:OfficeDocumentSettings><o:AllowPNG/><o:PixelsPerInch>96</o:PixelsPerInch></o:OfficeDocumentSettings></xml><![endif]--><style>body{background-color:#fff}.gse_alrt_title{text-decoration:none}.gse_alrt_title:hover{text-decoration:underline} @media screen and (max-width: 599px) {.gse_alrt_sni br{display:none;}}</style></head><body><!--[if gte mso 9]><table cellpadding="0" cellspacing="0" border="0"><tr><td style="width:600px"><![endif]--><div style="font-family:arial,sans-serif;font-size:13px;line-height:16px;color:#222;width:100%;max-width:600px"><h3 style="font-weight:lighter;font-size:18px;line-height:20px;"></h3><h3 style="font-weight:normal;margin:0;font-size:17px;line-height:20px;"><a href="https://scholar.google.com/scholar_url?url=https://www.earticle.net/Article/A476058&amp;hl=zh-CN&amp;sa=X&amp;d=15449677320770934050&amp;ei=XbIzaYyqBdOyieoP6r6mkQg&amp;scisig=ALhkC2Qpg8ABgGA497aFAFWPqseo&amp;oi=scholaralrt&amp;hist=gAQuJI0AAAAJ:6686011228539633326:ALhkC2TgVoGsEvMZexC23w76Vvzn&amp;html=&amp;pos=0&amp;folt=kw-top" class="gse_alrt_title" style="font-size:17px;color:#1a0dab;line-height:22px">The Representation–Presence–Embodiment Framework of <font color=#DD4B39>Teleoperated </font>Avatar <font color=#DD4B39>Robots</font>: An Empirical Model for Disability Telepresence</a></h3><div style="color:#006621;line-height:18px">Y Chang, OKD Lee, J Park, S Lee - 한국경영정보학회정기학술대회, 2025</div><div class="gse_alrt_sni" style="line-height:17px">… 영어 <font color=#DD4B39>Teleoperated</font> avatar <font color=#DD4B39>robots</font> can enable situated participation for individuals <br>
with mobility … A preregistered mixed-methods experiment in South Korea <br>
compares avatar <font color=#DD4B39>robots</font> with … offers design insights for inclusive telepresence in …</div><div style="width:auto"><table cellpadding="0" cellspacing="0" border="0"><tbody><tr><td style="padding-right:4px;mso-padding-alt:4px 16px 0 0;"><a href="https://scholar.google.com/citations?hl=zh-CN&amp;update_op=email_library_add&amp;info=ItX11qpHaNYJ&amp;citsig=AKwN59oAAAAAaxTl3eL7UmFlMrcbwpLh-1ChQug" style="text-decoration:none;display:inline-block;padding:4px 8px 4px 0;mso-padding-alt:0;"><img alt="保存" src="https://scholar.google.com/intl/zh-CN/scholar/images/1x/save-32.png" border="0" height="16" width="16" style="vertical-align:top"></a></td><td style="padding-right:4px;mso-padding-alt:4px 16px 0 0;"><a href="https://scholar.google.com/scholar_share?hl=zh-CN&amp;oi=scholaralrt&amp;ss=tw&amp;url=https://www.earticle.net/Article/A476058&amp;rt=The+Representation%E2%80%93Presence%E2%80%93Embodiment+Framework+of+Teleoperated+Avatar+Robots:+An+Empirical+Model+for+Disability+Telepresence&amp;scisig=ALhkC2SHODJWMiQEUmU-0onT1tkd" style="text-decoration:none;display:inline-block;padding:4px 8px;mso-padding-alt:0;"><img alt="Twitter" src="https://scholar.google.com/intl/zh-CN/scholar/images/1x/tw-32.png" border="0" height="16" width="16" style="vertical-align:top"></a></td><td style="padding-right:4px;mso-padding-alt:4px 16px 0 0;"><a href="https://scholar.google.com/scholar_share?hl=zh-CN&amp;oi=scholaralrt&amp;ss=in&amp;url=https://www.earticle.net/Article/A476058&amp;rt=The+Representation%E2%80%93Presence%E2%80%93Embodiment+Framework+of+Teleoperated+Avatar+Robots:+An+Empirical+Model+for+Disability+Telepresence&amp;scisig=ALhkC2SHODJWMiQEUmU-0onT1tkd" style="text-decoration:none;display:inline-block;padding:4px 8px;mso-padding-alt:0;"><img alt="LinkedIn" src="https://scholar.google.com/intl/zh-CN/scholar/images/1x/in-32.png" border="0" height="16" width="16" style="vertical-align:top"></a></td><td style="padding-right:4px;mso-padding-alt:4px 16px 0 0;"><a href="https://scholar.google.com/scholar_share?hl=zh-CN&amp;oi=scholaralrt&amp;ss=fb&amp;url=https://www.earticle.net/Article/A476058&amp;rt=The+Representation%E2%80%93Presence%E2%80%93Embodiment+Framework+of+Teleoperated+Avatar+Robots:+An+Empirical+Model+for+Disability+Telepresence&amp;scisig=ALhkC2SHODJWMiQEUmU-0onT1tkd" style="text-decoration:none;display:inline-block;padding:4px 8px;mso-padding-alt:0;"><img alt="Facebook" src="https://scholar.google.com/intl/zh-CN/scholar/images/1x/fb-32.png" border="0" height="16" width="16" style="vertical-align:top"></a></td></tr></tbody></table></div><br><h3 style="font-weight:normal;margin:0;font-size:17px;line-height:20px;"><span style="font-size:13px;font-weight:normal;color:#1a0dab;vertical-align:2px">[PDF]</span> <a href="https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2512.02729&amp;hl=zh-CN&amp;sa=X&amp;d=6063746883014571780&amp;ei=XbIzaYyqBdOyieoP6r6mkQg&amp;scisig=ALhkC2R9jc9hRgAihYZHKNwEKPSa&amp;oi=scholaralrt&amp;hist=gAQuJI0AAAAJ:6686011228539633326:ALhkC2TgVoGsEvMZexC23w76Vvzn&amp;html=&amp;pos=1&amp;folt=kw-top" class="gse_alrt_title" style="font-size:17px;color:#1a0dab;line-height:22px">RoboWheel: A Data Engine from Real-World Human Demonstrations for Cross-Embodiment <font color=#DD4B39>Robotic </font>Learning</a></h3><div style="color:#006621;line-height:18px">Y Zhang, Z Gao, S Li, LH Chen, K Liu, R Cheng, X Lin… - arXiv preprint arXiv …, 2025</div><div class="gse_alrt_sni" style="line-height:17px">… <font color=#DD4B39>robot</font> embodiments—even across domains—while preserving interaction <br>
semantics; and (iii) maintaining scalability by enabling ef- … Existing paradigms for <br>
<font color=#DD4B39>robot</font> data collection—<font color=#DD4B39>teleoperation</font> and simulated data generation—fail to meet …</div><div style="width:auto"><table cellpadding="0" cellspacing="0" border="0"><tbody><tr><td style="padding-right:4px;mso-padding-alt:4px 16px 0 0;"><a href="https://scholar.google.com/citations?hl=zh-CN&amp;update_op=email_library_add&amp;info=BDf3uanBJlQJ&amp;citsig=AKwN59oAAAAAaxTl3SFcDrVeKFoxANM8KR3JWyc" style="text-decoration:none;display:inline-block;padding:4px 8px 4px 0;mso-padding-alt:0;"><img alt="保存" src="https://scholar.google.com/intl/zh-CN/scholar/images/1x/save-32.png" border="0" height="16" width="16" style="vertical-align:top"></a></td><td style="padding-right:4px;mso-padding-alt:4px 16px 0 0;"><a href="https://scholar.google.com/scholar_share?hl=zh-CN&amp;oi=scholaralrt&amp;ss=tw&amp;url=https://arxiv.org/pdf/2512.02729&amp;rt=RoboWheel:+A+Data+Engine+from+Real-World+Human+Demonstrations+for+Cross-Embodiment+Robotic+Learning&amp;scisig=ALhkC2SyUyT1vOfShIKNK3qL3zr_" style="text-decoration:none;display:inline-block;padding:4px 8px;mso-padding-alt:0;"><img alt="Twitter" src="https://scholar.google.com/intl/zh-CN/scholar/images/1x/tw-32.png" border="0" height="16" width="16" style="vertical-align:top"></a></td><td style="padding-right:4px;mso-padding-alt:4px 16px 0 0;"><a href="https://scholar.google.com/scholar_share?hl=zh-CN&amp;oi=scholaralrt&amp;ss=in&amp;url=https://arxiv.org/pdf/2512.02729&amp;rt=RoboWheel:+A+Data+Engine+from+Real-World+Human+Demonstrations+for+Cross-Embodiment+Robotic+Learning&amp;scisig=ALhkC2SyUyT1vOfShIKNK3qL3zr_" style="text-decoration:none;display:inline-block;padding:4px 8px;mso-padding-alt:0;"><img alt="LinkedIn" src="https://scholar.google.com/intl/zh-CN/scholar/images/1x/in-32.png" border="0" height="16" width="16" style="vertical-align:top"></a></td><td style="padding-right:4px;mso-padding-alt:4px 16px 0 0;"><a href="https://scholar.google.com/scholar_share?hl=zh-CN&amp;oi=scholaralrt&amp;ss=fb&amp;url=https://arxiv.org/pdf/2512.02729&amp;rt=RoboWheel:+A+Data+Engine+from+Real-World+Human+Demonstrations+for+Cross-Embodiment+Robotic+Learning&amp;scisig=ALhkC2SyUyT1vOfShIKNK3qL3zr_" style="text-decoration:none;display:inline-block;padding:4px 8px;mso-padding-alt:0;"><img alt="Facebook" src="https://scholar.google.com/intl/zh-CN/scholar/images/1x/fb-32.png" border="0" height="16" width="16" style="vertical-align:top"></a></td></tr></tbody></table></div><br><h3 style="font-weight:normal;margin:0;font-size:17px;line-height:20px;"><span style="font-size:13px;font-weight:normal;color:#1a0dab;vertical-align:2px">[PDF]</span> <a href="https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2512.02787&amp;hl=zh-CN&amp;sa=X&amp;d=15246711878236000224&amp;ei=XbIzaYyqBdOyieoP6r6mkQg&amp;scisig=ALhkC2Q80jLyNaQBEYmplHYT9jNh&amp;oi=scholaralrt&amp;hist=gAQuJI0AAAAJ:6686011228539633326:ALhkC2TgVoGsEvMZexC23w76Vvzn&amp;html=&amp;pos=2&amp;folt=kw-top" class="gse_alrt_title" style="font-size:17px;color:#1a0dab;line-height:22px">Diagnose, Correct, and Learn from Manipulation Failures via Visual Symbols</a></h3><div style="color:#006621;line-height:18px">X Zeng, X Zhou, Y Li, J Shi, T Li, L Chen, L Ren, YL Li - arXiv preprint arXiv …, 2025</div><div class="gse_alrt_sni" style="line-height:17px">… In fact, during <font color=#DD4B39>teleoperation</font> data collection or policy rollout, it is common for <br>
<font color=#DD4B39>robots</font> to … To leverage real-world failure data, we propose a framework for <font color=#DD4B39>robotic</font> <br>
manipulation failure … Left: We collect real-world manipulation trajectories via …</div><div style="width:auto"><table cellpadding="0" cellspacing="0" border="0"><tbody><tr><td style="padding-right:4px;mso-padding-alt:4px 16px 0 0;"><a href="https://scholar.google.com/citations?hl=zh-CN&amp;update_op=email_library_add&amp;info=4E_8Nqwzl9MJ&amp;citsig=AKwN59oAAAAAaxTl3Q1c8NPBlteOEBf-7--QtvQ" style="text-decoration:none;display:inline-block;padding:4px 8px 4px 0;mso-padding-alt:0;"><img alt="保存" src="https://scholar.google.com/intl/zh-CN/scholar/images/1x/save-32.png" border="0" height="16" width="16" style="vertical-align:top"></a></td><td style="padding-right:4px;mso-padding-alt:4px 16px 0 0;"><a href="https://scholar.google.com/scholar_share?hl=zh-CN&amp;oi=scholaralrt&amp;ss=tw&amp;url=https://arxiv.org/pdf/2512.02787&amp;rt=Diagnose,+Correct,+and+Learn+from+Manipulation+Failures+via+Visual+Symbols&amp;scisig=ALhkC2Srds-HJ1OdD7OVwi2qoDLg" style="text-decoration:none;display:inline-block;padding:4px 8px;mso-padding-alt:0;"><img alt="Twitter" src="https://scholar.google.com/intl/zh-CN/scholar/images/1x/tw-32.png" border="0" height="16" width="16" style="vertical-align:top"></a></td><td style="padding-right:4px;mso-padding-alt:4px 16px 0 0;"><a href="https://scholar.google.com/scholar_share?hl=zh-CN&amp;oi=scholaralrt&amp;ss=in&amp;url=https://arxiv.org/pdf/2512.02787&amp;rt=Diagnose,+Correct,+and+Learn+from+Manipulation+Failures+via+Visual+Symbols&amp;scisig=ALhkC2Srds-HJ1OdD7OVwi2qoDLg" style="text-decoration:none;display:inline-block;padding:4px 8px;mso-padding-alt:0;"><img alt="LinkedIn" src="https://scholar.google.com/intl/zh-CN/scholar/images/1x/in-32.png" border="0" height="16" width="16" style="vertical-align:top"></a></td><td style="padding-right:4px;mso-padding-alt:4px 16px 0 0;"><a href="https://scholar.google.com/scholar_share?hl=zh-CN&amp;oi=scholaralrt&amp;ss=fb&amp;url=https://arxiv.org/pdf/2512.02787&amp;rt=Diagnose,+Correct,+and+Learn+from+Manipulation+Failures+via+Visual+Symbols&amp;scisig=ALhkC2Srds-HJ1OdD7OVwi2qoDLg" style="text-decoration:none;display:inline-block;padding:4px 8px;mso-padding-alt:0;"><img alt="Facebook" src="https://scholar.google.com/intl/zh-CN/scholar/images/1x/fb-32.png" border="0" height="16" width="16" style="vertical-align:top"></a></td></tr></tbody></table></div><br><h3 style="font-weight:normal;margin:0;font-size:17px;line-height:20px;"><span style="font-size:13px;font-weight:normal;color:#1a0dab;vertical-align:2px">[PDF]</span> <a href="https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2512.02834&amp;hl=zh-CN&amp;sa=X&amp;d=2369683172664435385&amp;ei=XbIzaYyqBdOyieoP6r6mkQg&amp;scisig=ALhkC2QU7RRAQ-OAxMrD7fFSCsu_&amp;oi=scholaralrt&amp;hist=gAQuJI0AAAAJ:6686011228539633326:ALhkC2TgVoGsEvMZexC23w76Vvzn&amp;html=&amp;pos=3&amp;folt=kw-top" class="gse_alrt_title" style="font-size:17px;color:#1a0dab;line-height:22px">Steering Vision-Language-Action Models as Anti-Exploration: A Test-Time Scaling Approach</a></h3><div style="color:#006621;line-height:18px">S Yang, Y Zhang, H He, L Pan, X Li, C Bai, X Li - arXiv preprint arXiv:2512.02834, 2025</div><div class="gse_alrt_sni" style="line-height:17px">… We consider a language-conditioned <font color=#DD4B39>robotic</font> manipulation task with vision input <br>
under the imitation learning setting. We assume access to … For data collection, we <br>
perform <font color=#DD4B39>teleoperation</font> to record 100 episodes for each task. The collected data are …</div><div style="width:auto"><table cellpadding="0" cellspacing="0" border="0"><tbody><tr><td style="padding-right:4px;mso-padding-alt:4px 16px 0 0;"><a href="https://scholar.google.com/citations?hl=zh-CN&amp;update_op=email_library_add&amp;info=uWLiWErO4iAJ&amp;citsig=AKwN59oAAAAAaxTl3Vz_zh7o0wN8in91M1xMpiY" style="text-decoration:none;display:inline-block;padding:4px 8px 4px 0;mso-padding-alt:0;"><img alt="保存" src="https://scholar.google.com/intl/zh-CN/scholar/images/1x/save-32.png" border="0" height="16" width="16" style="vertical-align:top"></a></td><td style="padding-right:4px;mso-padding-alt:4px 16px 0 0;"><a href="https://scholar.google.com/scholar_share?hl=zh-CN&amp;oi=scholaralrt&amp;ss=tw&amp;url=https://arxiv.org/pdf/2512.02834&amp;rt=Steering+Vision-Language-Action+Models+as+Anti-Exploration:+A+Test-Time+Scaling+Approach&amp;scisig=ALhkC2S33yDp1OVlCsp_Uneifv9V" style="text-decoration:none;display:inline-block;padding:4px 8px;mso-padding-alt:0;"><img alt="Twitter" src="https://scholar.google.com/intl/zh-CN/scholar/images/1x/tw-32.png" border="0" height="16" width="16" style="vertical-align:top"></a></td><td style="padding-right:4px;mso-padding-alt:4px 16px 0 0;"><a href="https://scholar.google.com/scholar_share?hl=zh-CN&amp;oi=scholaralrt&amp;ss=in&amp;url=https://arxiv.org/pdf/2512.02834&amp;rt=Steering+Vision-Language-Action+Models+as+Anti-Exploration:+A+Test-Time+Scaling+Approach&amp;scisig=ALhkC2S33yDp1OVlCsp_Uneifv9V" style="text-decoration:none;display:inline-block;padding:4px 8px;mso-padding-alt:0;"><img alt="LinkedIn" src="https://scholar.google.com/intl/zh-CN/scholar/images/1x/in-32.png" border="0" height="16" width="16" style="vertical-align:top"></a></td><td style="padding-right:4px;mso-padding-alt:4px 16px 0 0;"><a href="https://scholar.google.com/scholar_share?hl=zh-CN&amp;oi=scholaralrt&amp;ss=fb&amp;url=https://arxiv.org/pdf/2512.02834&amp;rt=Steering+Vision-Language-Action+Models+as+Anti-Exploration:+A+Test-Time+Scaling+Approach&amp;scisig=ALhkC2S33yDp1OVlCsp_Uneifv9V" style="text-decoration:none;display:inline-block;padding:4px 8px;mso-padding-alt:0;"><img alt="Facebook" src="https://scholar.google.com/intl/zh-CN/scholar/images/1x/fb-32.png" border="0" height="16" width="16" style="vertical-align:top"></a></td></tr></tbody></table></div><br><h3 style="font-weight:normal;font-size:18px;line-height:20px;"></h3><div style="line-height:16px;mso-line-height-rule:exactly;border-top:1px solid #bdbdbd">&nbsp;</div><p style="margin:8px 0 16px 0;color:#666">Google 学术搜索发送此邮件，是因为您关注了 <a href="https://scholar.google.com/scholar?q=teleoperation+robot&amp;as_sdt=0&amp;scisbd=1&amp;hl=zh-CN" style="color:#1a0dab;">[teleoperation robot]</a> 的新搜索结果。<img src="https://scholar.google.com/scholar_url?url=https://scholar.google.com/scholar/images/cleardot.gif&amp;hl=zh-CN&amp;sa=X&amp;ei=XbIzaYyqBdOyieoP6r6mkQg&amp;scisig=ALhkC2RPFLZ1AgVl4R-yxCYLTgfw&amp;hist=gAQuJI0AAAAJ:6686011228539633326:ALhkC2TgVoGsEvMZexC23w76Vvzn&amp;html=&amp;folt=kw-top&amp;trs=0,1,2,3" height=1 width=1 alt=""></p><div style="margin-bottom:8px;"><div><!--[if gte mso 9]><table border="0" cellspacing="0" cellpadding="0"><tr><td style="mso-line-height-rule:exactly;line-height:27px;border-top:1px solid #fff;border-bottom:1px solid #fff;mso-text-raise:-1px;"><![endif]--><a href="https://scholar.google.com/scholar_alerts?view_op=list_alerts&amp;email_for_op=ligen4073187%40gmail.com&amp;alert_id=rr5g0cl7yVwJ&amp;hl=zh-CN" style="display:inline-block;text-decoration:none;font-family:arial,sans-serif;font-size:13px;font-size:13px;font-size:15px;line-height:21px;padding:3px 0;color:#1a0dab;border-top:1px solid transparent;border-bottom:1px solid transparent;border-radius:3px;mso-padding-alt:0;mso-border-alt:none;"><span style="mso-text-raise:5px">列出快讯</span></a><!--[if gte mso 9]></td></tr></table><![endif]--></div><div><!--[if gte mso 9]><table border="0" cellspacing="0" cellpadding="0"><tr><td style="mso-line-height-rule:exactly;line-height:27px;border-top:1px solid #fff;border-bottom:1px solid #fff;mso-text-raise:-1px;"><![endif]--><a href="https://scholar.google.com/scholar_alerts?view_op=cancel_alert_options&amp;email_for_op=ligen4073187%40gmail.com&amp;alert_id=rr5g0cl7yVwJ&amp;hl=zh-CN&amp;citsig=AKwN59oAAAAAaUYnXSPxrqDEtY0dV4qQQzEiuTE" style="display:inline-block;text-decoration:none;font-family:arial,sans-serif;font-size:13px;font-size:13px;font-size:15px;line-height:21px;padding:3px 0;color:#1a0dab;border-top:1px solid transparent;border-bottom:1px solid transparent;border-radius:3px;mso-padding-alt:0;mso-border-alt:none;"><span style="mso-text-raise:5px">取消快讯</span></a><!--[if gte mso 9]></td></tr></table><![endif]--></div></div></div><!--[if gte mso 9]></td></tr></table><![endif]--></body></html>
