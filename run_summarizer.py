#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Paper Summarizer - è‡ªåŠ¨æ€»ç»“Googleå­¦æœ¯é‚®ä»¶æ¨é€
"""

import os
import imaplib
import email
from email.header import decode_header
import re
from datetime import datetime, timedelta
import yaml
from dotenv import load_dotenv
import ollama
from bs4 import BeautifulSoup
import time
import ssl
from crewai import Agent, Task, Crew, LLM
import logging
import pandas as pd

# ç¦ç”¨ CrewAI é¥æµ‹ï¼ˆå¯é€‰ï¼‰
os.environ['CREWAI_TELEMETRY_OPT_OUT'] = 'true'
os.environ['OTEL_SDK_DISABLED'] = 'true'

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# é…ç½®
# GMAIL_USER = os.getenv('GMAIL_USER')
# GMAIL_PASSWORD = os.getenv('GMAIL_PASSWORD')
QMAIL_USER = os.getenv('QMAIL_USER')
QMAIL_PASSWORD = os.getenv('QMAIL_PASSWORD')
OLLAMA_MODEL = os.getenv('OLLAMA_MODEL', 'qwen2.5:32b')
# MAX_EMAILS = int(os.getenv('MAX_EMAILS', 20))
MAX_EMAILS = 30
OLLAMA_BASE_URL = os.getenv('OLLAMA_BASE_URL', 'http://localhost:11434')
# æ—¥æœŸèŒƒå›´é…ç½®ï¼šä»å‰START_DAYSå¤©åˆ°å‰END_DAYSå¤©
# ä¾‹å¦‚ï¼šSTART_DAYS=3, END_DAYS=0 è¡¨ç¤ºä»å‰3å¤©åˆ°ä»Šå¤©
#      START_DAYS=7, END_DAYS=3 è¡¨ç¤ºä»å‰7å¤©åˆ°å‰3å¤©
START_DAYS = int(os.getenv('START_DAYS', 1))  # é»˜è®¤ä»å‰1å¤©å¼€å§‹
END_DAYS = int(os.getenv('END_DAYS', 0))  # é»˜è®¤åˆ°ä»Šå¤©ï¼ˆå‰0å¤©ï¼‰
# START_DAYS = 1  # é»˜è®¤ä»å‰1å¤©å¼€å§‹
# END_DAYS = 0  # é»˜è®¤åˆ°ä»Šå¤©ï¼ˆå‰0å¤©ï¼‰
# å¤‡ä»½è·¯å¾„é…ç½®ï¼ˆå¯é€‰ï¼‰ï¼šå¦‚æœè®¾ç½®äº†æ­¤è·¯å¾„ï¼ŒæŠ¥å‘Šä¼šåŒæ—¶ä¿å­˜åˆ°è¯¥è·¯å¾„
BACKUP_DIR = os.getenv('BACKUP_DIR', '')  # é»˜è®¤ä¸ºç©ºï¼Œä¸è¿›è¡Œå¤‡ä»½

# è®¾ç½®ç¯å¢ƒå˜é‡ï¼ˆCrewAI é€šè¿‡ LiteLLM è¿æ¥ Ollama éœ€è¦è¿™äº›ï¼‰
os.environ['OLLAMA_API_BASE'] = OLLAMA_BASE_URL
if not os.getenv('OPENAI_API_KEY'):
    os.environ['OPENAI_API_KEY'] = 'ollama'  # å ä½ç¬¦ï¼Œå®é™…ä¸ä½¿ç”¨

# åˆå§‹åŒ– CrewAI LLM
# å…³é”®ï¼šæ¨¡å‹åç§°å¿…é¡»åŒ…å« "ollama/" å‰ç¼€
llm_model_name = f"ollama/{OLLAMA_MODEL}" if not OLLAMA_MODEL.startswith("ollama/") else OLLAMA_MODEL

logging.info(f"åˆå§‹åŒ– CrewAI LLM: model={llm_model_name}, base_url={OLLAMA_BASE_URL}")

llm = LLM(
    model=llm_model_name,
    base_url=OLLAMA_BASE_URL,
    api_key="ollama"  # Ollama ä¸éœ€è¦çœŸå®çš„ API key
)

# åŠ è½½å…³é”®è¯
with open('keywords.yaml', 'r', encoding='utf-8') as f:
    KEYWORDS = yaml.safe_load(f)

HIGH_PRIORITY_KEYWORDS = [kw.lower() for kw in KEYWORDS['high_priority']]
RELATED_KEYWORDS = [kw.lower() for kw in KEYWORDS['related']]


def connect_gmail(max_retries=3, retry_delay=5):
    """è¿æ¥Gmail IMAPæœåŠ¡å™¨ï¼Œå¸¦é‡è¯•æœºåˆ¶"""
    print("æ­£åœ¨è¿æ¥Gmail...")
    
    for attempt in range(max_retries):
        try:
            # åˆ›å»º SSL ä¸Šä¸‹æ–‡
            context = ssl.create_default_context()
            
            # ä½¿ç”¨è¶…æ—¶è®¾ç½®è¿æ¥
            # mail = imaplib.IMAP4_SSL("imap.gmail.com", port=993, ssl_context=context)
            # mail.sock.settimeout(30)  # è®¾ç½®30ç§’è¶…æ—¶
            
            # mail.login(GMAIL_USER, GMAIL_PASSWORD)
            # print("âœ“ Gmailè¿æ¥æˆåŠŸ")

            mail = imaplib.IMAP4_SSL("imap.qq.com", port=993, ssl_context=context)
            mail.sock.settimeout(30)  # è®¾ç½®30ç§’è¶…æ—¶
            
            mail.login(QMAIL_USER, QMAIL_PASSWORD)
            print("âœ“ QQmailè¿æ¥æˆåŠŸ")
            return mail
            
        except (imaplib.IMAP4.error, ssl.SSLError, OSError) as e:
            if attempt < max_retries - 1:
                print(f"è¿æ¥å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {str(e)}")
                print(f"ç­‰å¾… {retry_delay} ç§’åé‡è¯•...")
                time.sleep(retry_delay)
            else:
                print(f"âœ— QQmailè¿æ¥å¤±è´¥ï¼Œå·²é‡è¯• {max_retries} æ¬¡")
                raise Exception(f"æ— æ³•è¿æ¥åˆ°Gmail: {str(e)}")
    
    raise Exception("æ— æ³•è¿æ¥åˆ°Gmail")


def parse_email_date(date_str):
    """è§£æé‚®ä»¶æ—¥æœŸå­—ç¬¦ä¸²ä¸ºdatetimeå¯¹è±¡"""
    try:
        # ä½¿ç”¨email.utilsçš„æ ‡å‡†æ–¹æ³•è§£æé‚®ä»¶æ—¥æœŸ
        from email.utils import parsedate_to_datetime
        return parsedate_to_datetime(date_str)
    except (ValueError, TypeError, AttributeError):
        # å¦‚æœæ ‡å‡†æ–¹æ³•å¤±è´¥ï¼Œè¿”å›None
        return None


def is_email_in_date_range(msg, start_days=1, end_days=0):
    """
    æ£€æŸ¥é‚®ä»¶æ˜¯å¦åœ¨æŒ‡å®šçš„æ—¥æœŸèŒƒå›´å†…
    
    Args:
        msg: é‚®ä»¶å¯¹è±¡
        start_days: å¼€å§‹æ—¥æœŸï¼ˆå‰start_dayså¤©ï¼Œä¾‹å¦‚start_days=3è¡¨ç¤ºå‰3å¤©ï¼‰
        end_days: ç»“æŸæ—¥æœŸï¼ˆå‰end_dayså¤©ï¼Œä¾‹å¦‚end_days=0è¡¨ç¤ºä»Šå¤©ï¼Œend_days=1è¡¨ç¤ºæ˜¨å¤©ï¼‰
    
    Returns:
        bool: å¦‚æœé‚®ä»¶åœ¨æ—¥æœŸèŒƒå›´å†…è¿”å›Trueï¼Œå¦åˆ™è¿”å›False
    """
    try:
        # è·å–é‚®ä»¶æ—¥æœŸ
        date_str = msg.get('Date')
        if not date_str:
            return False
        
        email_date = parse_email_date(date_str)
        if not email_date:
            return False
        
        # è®¡ç®—æ—¥æœŸèŒƒå›´ï¼ˆå‰start_dayså¤©åˆ°å‰end_dayså¤©ï¼‰
        now = datetime.now()
        # ç»“æŸæ—¥æœŸï¼šå‰end_dayså¤©ï¼ˆä¸åŒ…å«ä¸‹ä¸€å¤©ï¼‰
        end_date = (now - timedelta(days=end_days)).date()
        end_date_exclusive = end_date + timedelta(days=1)
        # å¼€å§‹æ—¥æœŸï¼šå‰start_dayså¤©
        start_date = (now - timedelta(days=start_days)).date()
        
        # åªæ¯”è¾ƒæ—¥æœŸéƒ¨åˆ†ï¼Œå¿½ç•¥æ—¶é—´
        email_date_only = email_date.date()
        
        return start_date <= email_date_only < end_date_exclusive
    except Exception as e:
        logging.warning(f"æ£€æŸ¥é‚®ä»¶æ—¥æœŸæ—¶å‡ºé”™: {str(e)}")
        return True  # å¦‚æœæ— æ³•è§£ææ—¥æœŸï¼Œé»˜è®¤åŒ…å«è¯¥é‚®ä»¶


def fetch_scholar_emails(mail, start_days=1, end_days=0):
    """
    è·å–Googleå­¦æœ¯æ¨é€é‚®ä»¶
    
    Args:
        mail: IMAPé‚®ä»¶è¿æ¥å¯¹è±¡
        start_days: å¼€å§‹æ—¥æœŸï¼ˆå‰start_dayså¤©ï¼Œä¾‹å¦‚start_days=3è¡¨ç¤ºä»å‰3å¤©å¼€å§‹ï¼‰
        end_days: ç»“æŸæ—¥æœŸï¼ˆå‰end_dayså¤©ï¼Œä¾‹å¦‚end_days=0è¡¨ç¤ºåˆ°ä»Šå¤©ï¼Œend_days=1è¡¨ç¤ºåˆ°æ˜¨å¤©ï¼‰
    
    Returns:
        list: é‚®ä»¶IDåˆ—è¡¨
    """
    now = datetime.now()
    start_date_obj = now - timedelta(days=start_days)
    end_date_obj = now - timedelta(days=end_days)
    end_date_exclusive = end_date_obj + timedelta(days=1)
    
    start_date_str = start_date_obj.strftime("%d-%b-%Y")
    end_date_str = end_date_exclusive.strftime("%d-%b-%Y")
    
    if start_days == end_days:
        print(f"\næ­£åœ¨è·å–å‰{start_days}å¤©çš„Googleå­¦æœ¯æ¨é€...")
    else:
        print(f"\næ­£åœ¨è·å–ä»å‰{start_days}å¤©åˆ°å‰{end_days}å¤©çš„Googleå­¦æœ¯æ¨é€...")
    
    # é€‰æ‹©æ”¶ä»¶ç®±
    mail.select("inbox")
    
    # æœç´¢Googleå­¦æœ¯é‚®ä»¶ï¼Œä½¿ç”¨SINCEå’ŒBEFOREé™åˆ¶æ—¥æœŸèŒƒå›´
    # search_criteria = f'(FROM "ligen4073187@gmail.com" SINCE {since_date}) AND (HEADER FROM "scholaralerts-noreply@google.com")'
    search_criteria = f'(FROM "scholaralerts-noreply@google.com" SINCE {start_date_str} BEFORE {end_date_str})'
    status, messages = mail.search(None, search_criteria)
    
    email_ids = messages[0].split()
    date_range_str = f"{start_date_obj.strftime('%Y-%m-%d')} åˆ° {end_date_obj.strftime('%Y-%m-%d')}"
    print(f"âœ“ æ‰¾åˆ° {len(email_ids)} å°é‚®ä»¶ï¼ˆæ—¥æœŸèŒƒå›´: {date_range_str}ï¼‰")
    
    return email_ids


def extract_paper_info(email_body):
    """ä»é‚®ä»¶ä¸­æå–è®ºæ–‡ä¿¡æ¯"""
    soup = BeautifulSoup(email_body, 'html.parser')
    
    papers = []
    
    # Googleå­¦æœ¯æ¨é€çš„ç»“æ„é€šå¸¸åŒ…å«å¤šç¯‡è®ºæ–‡
    # æŸ¥æ‰¾æ‰€æœ‰è®ºæ–‡æ ‡é¢˜å’Œé“¾æ¥
    for h3 in soup.find_all('h3'):
        title_link = h3.find('a')
        if title_link:
            title = title_link.get_text(strip=True)
            link = title_link.get('href', '')
            
            # æŸ¥æ‰¾ä½œè€…å’Œæ‘˜è¦ä¿¡æ¯
            parent = h3.find_parent()
            if parent:
                text_content = parent.get_text()
                
                paper = {
                    'title': title,
                    'link': link,
                    'snippet': text_content[:500]  # è·å–å‰500å­—ç¬¦ä½œä¸ºç‰‡æ®µ
                }
                papers.append(paper)
    
    return papers


def check_relevance(paper):
    """æ£€æŸ¥è®ºæ–‡ç›¸å…³æ€§"""
    text = (paper['title'] + ' ' + paper['snippet']).lower()
    
    # æ£€æŸ¥é«˜ä¼˜å…ˆçº§å…³é”®è¯
    high_priority_matches = sum(1 for kw in HIGH_PRIORITY_KEYWORDS if kw in text)
    
    # æ£€æŸ¥ç›¸å…³å…³é”®è¯
    related_matches = sum(1 for kw in RELATED_KEYWORDS if kw in text)
    
    # è®¡ç®—ç›¸å…³æ€§åˆ†æ•°
    relevance_score = high_priority_matches * 2 + related_matches
    
    return relevance_score, high_priority_matches > 0


def create_translator_agent():
    """åˆ›å»ºä¸“ä¸šç¿»è¯‘ Agent"""
    return Agent(
        role="ä¸“ä¸šç¿»è¯‘ä¸“å®¶",
        goal="å°†è‹±æ–‡è®ºæ–‡å†…å®¹å‡†ç¡®ã€ä¸“ä¸šåœ°ç¿»è¯‘æˆä¸­æ–‡ï¼Œç¡®ä¿ä¸“ä¸šæœ¯è¯­çš„å‡†ç¡®æ€§å’ŒæŠ€æœ¯è¡¨è¾¾çš„æ¸…æ™°æ€§",
        backstory="ä½ æ˜¯ä¸€ä½åœ¨æœºå™¨äººå­¦ã€æ§åˆ¶ç†è®ºã€é¥æ“ä½œã€æœºå™¨äººåŠ¨åŠ›å­¦å’ŒåŠ›æ§é¢†åŸŸæ‹¥æœ‰æ·±åšä¸“ä¸šèƒŒæ™¯çš„ç¿»è¯‘ä¸“å®¶ã€‚ä½ æ“…é•¿å°†è‹±æ–‡å­¦æœ¯è®ºæ–‡ç¿»è¯‘æˆä¸­æ–‡ï¼Œèƒ½å¤Ÿå‡†ç¡®å¤„ç†ä¸“ä¸šæœ¯è¯­ï¼Œä¿æŒæŠ€æœ¯æè¿°çš„å®Œæ•´æ€§å’Œé€»è¾‘ç»“æ„çš„æ¸…æ™°æ€§ã€‚",
        allow_delegation=False,
        verbose=True,
        llm=llm,
        max_iter=3,
        max_execution_time=300
    )


def create_reviewer_agent():
    """åˆ›å»ºä¸“ä¸šè¯„å®¡ Agent"""
    return Agent(
        role="ä¸“ä¸šè¯„å®¡ä¸“å®¶",
        goal="å¯¹è®ºæ–‡è¿›è¡Œä¸“ä¸šè¯„å®¡ï¼Œç”Ÿæˆç»“æ„åŒ–æ€»ç»“å¹¶ç»™å‡ºç®€æ´çš„5åˆ†åˆ¶è¯„åˆ†ï¼ˆåªè¾“å‡ºä¸€æ¬¡ï¼‰",
        backstory="ä½ æ˜¯ä¸€ä½åœ¨æœºå™¨äººå­¦ã€æ§åˆ¶ç†è®ºã€é¥æ“ä½œã€æœºå™¨äººåŠ¨åŠ›å­¦å’ŒåŠ›æ§é¢†åŸŸæ‹¥æœ‰ä¸°å¯Œç ”ç©¶ç»éªŒçš„è¯„å®¡ä¸“å®¶ã€‚ä½ èƒ½å¤Ÿä»åˆ›æ–°æ€§ã€æŠ€æœ¯æ·±åº¦ã€ç›¸å…³æ€§ã€å®ç”¨æ€§å’Œç ”ç©¶è´¨é‡ç­‰å¤šä¸ªç»´åº¦å¯¹è®ºæ–‡è¿›è¡Œå®¢è§‚ã€ä¸“ä¸šçš„è¯„ä»·ã€‚ä½ æ€»æ˜¯ç®€æ´æ˜äº†åœ°è¾“å‡ºç»“æœï¼Œä¸ä¼šé‡å¤è¯´æ˜ã€‚",
        allow_delegation=False,
        verbose=True,
        llm=llm,
        max_iter=2,  # å‡å°‘è¿­ä»£æ¬¡æ•°ï¼Œé¿å…é‡å¤
        max_execution_time=300
    )


def create_translation_task(paper):
    """åˆ›å»ºç¿»è¯‘ä»»åŠ¡"""
    return Task(
        description=(
            f"è¯·å°†ä»¥ä¸‹è‹±æ–‡è®ºæ–‡ä¿¡æ¯å‡†ç¡®ã€ä¸“ä¸šåœ°ç¿»è¯‘æˆä¸­æ–‡ã€‚\n\n"
            f"è®ºæ–‡æ ‡é¢˜ï¼š\n{paper['title']}\n\n"
            f"è®ºæ–‡ç‰‡æ®µï¼ˆè‹±æ–‡åŸæ–‡ï¼‰ï¼š\n{paper['snippet']}\n\n"
            f"ç¿»è¯‘è¦æ±‚ï¼š\n"
            f"1. ä¿æŒä¸“ä¸šæœ¯è¯­çš„å‡†ç¡®æ€§ï¼Œä½¿ç”¨è¯¥é¢†åŸŸæ ‡å‡†çš„ä¸­æ–‡æœ¯è¯­\n"
            f"2. ç¡®ä¿æŠ€æœ¯æè¿°çš„å‡†ç¡®æ€§å’Œå®Œæ•´æ€§\n"
            f"3. ä¿æŒåŸæ–‡çš„é€»è¾‘ç»“æ„å’Œè¡¨è¾¾é£æ ¼\n"
            f"4. å¦‚æœé‡åˆ°ä¸ç¡®å®šçš„æœ¯è¯­ï¼Œè¯·æä¾›æœ€å¯èƒ½çš„ä¸“ä¸šç¿»è¯‘\n\n"
            f"è¯·ç›´æ¥è¾“å‡ºç¿»è¯‘ç»“æœï¼Œä¸éœ€è¦é¢å¤–è¯´æ˜ã€‚"
        ),
        agent=create_translator_agent(),
        expected_output="ç¿»è¯‘åçš„ä¸­æ–‡å†…å®¹ï¼Œä¿æŒåŸæ–‡çš„ç»“æ„å’Œé€»è¾‘"
    )


def create_review_task(paper, translated_content):
    """åˆ›å»ºè¯„å®¡ä»»åŠ¡"""
    return Task(
        description=(
            f"è¯·å¯¹ä»¥ä¸‹è®ºæ–‡è¿›è¡Œä¸“ä¸šè¯„å®¡ï¼Œç”Ÿæˆç»“æ„åŒ–æ€»ç»“å¹¶ç»™å‡ºè¯„åˆ†ã€‚\n\n"
            f"è®ºæ–‡æ ‡é¢˜ï¼ˆè‹±æ–‡ï¼‰ï¼š\n{paper['title']}\n\n"
            f"è®ºæ–‡å†…å®¹ï¼ˆå·²ç¿»è¯‘ä¸ºä¸­æ–‡ï¼‰ï¼š\n{translated_content}\n\n"
            f"è¯·æŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼ˆä¸è¦é‡å¤è¯´æ˜è¯„åˆ†è§„åˆ™ï¼Œç›´æ¥ç»™å‡ºç»“æœï¼‰ï¼š\n\n"
            f"**æ ¸å¿ƒè´¡çŒ®**ï¼šï¼ˆ1-2å¥è¯è¯´æ˜ä¸»è¦åˆ›æ–°ç‚¹å’Œè´¡çŒ®ï¼‰\n\n"
            f"**æŠ€æœ¯æ–¹æ³•**ï¼šï¼ˆç®€è¿°ä¸»è¦æŠ€æœ¯è·¯çº¿å’Œæ–¹æ³•ï¼‰\n\n"
            f"**ç›¸å…³æ€§åˆ†æ**ï¼šï¼ˆè¯¦ç»†è¯´æ˜ä¸é¥æ“ä½œ/æœºå™¨äººåŠ¨åŠ›å­¦/åŠ›æ§/æœºå™¨äººæ§åˆ¶çš„å…³ç³»ï¼‰\n\n"
            f"**æŠ€æœ¯ä»·å€¼**ï¼šï¼ˆè¯„ä¼°è¯¥è®ºæ–‡çš„æŠ€æœ¯ä»·å€¼å’Œæ½œåœ¨åº”ç”¨ï¼‰\n\n"
            f"**å€¼å¾—å…³æ³¨çš„åŸå› **ï¼šï¼ˆä¸ºä»€ä¹ˆè¿™ç¯‡è®ºæ–‡é‡è¦ï¼Œæœ‰å“ªäº›äº®ç‚¹ï¼‰\n\n"
            f"**è¯„åˆ†è¯¦æƒ…**ï¼š\n"
            f"```json\n"
            f'{{"åˆ›æ–°æ€§": 0.0-1.0, "æŠ€æœ¯æ·±åº¦": 0.0-1.0, "ç›¸å…³æ€§": 0.0-1.0, "å®ç”¨æ€§": 0.0-1.0, "ç ”ç©¶è´¨é‡": 0.0-1.0, "æ€»åˆ†": 0.0-5.0, "è¯„åˆ†ç†ç”±": "ç®€è¦è¯´æ˜è¯„åˆ†ä¾æ®"}}\n'
            f"```\n\n"
            f"é‡è¦ï¼šè¯„åˆ†è¯¦æƒ…å¿…é¡»ä½¿ç”¨Markdownä»£ç å—æ ¼å¼ï¼ˆ```json ... ```ï¼‰ï¼Œåªè¾“å‡ºä¸€æ¬¡ï¼Œè¯„åˆ†ç†ç”±å¿…é¡»åŒ…å«åœ¨JSONä¸­ï¼Œä¸è¦é‡å¤è¯´æ˜è¯„åˆ†è§„åˆ™æˆ–å¤šæ¬¡è¾“å‡ºè¯„åˆ†ã€‚"
        ),
        agent=create_reviewer_agent(),
        expected_output=(
            "è¯„å®¡æŠ¥å‘ŠåŒ…å«ï¼šæ ¸å¿ƒè´¡çŒ®ã€æŠ€æœ¯æ–¹æ³•ã€ç›¸å…³æ€§åˆ†æã€æŠ€æœ¯ä»·å€¼ã€å€¼å¾—å…³æ³¨çš„åŸå› ï¼Œ"
            "ä»¥åŠä¸€ä¸ªMarkdownä»£ç å—æ ¼å¼çš„JSONè¯„åˆ†è¯¦æƒ…ï¼ˆåŒ…å«å„ç»´åº¦åˆ†æ•°ã€æ€»åˆ†å’Œè¯„åˆ†ç†ç”±ï¼Œä¸è¦é‡å¤è¾“å‡ºï¼‰ã€‚"
        )
    )


def process_paper_with_crewai(paper):
    """
    ä½¿ç”¨ CrewAI æ¡†æ¶å¤„ç†è®ºæ–‡ï¼šç¿»è¯‘ + è¯„å®¡
    è¿”å›å¤„ç†ç»“æœå­—å…¸
    """
    try:
        # æ­¥éª¤1: ç¿»è¯‘
        print("  [æ­¥éª¤1/2] ä¸“ä¸šç¿»è¯‘ä¸­...")
        translation_crew = Crew(
            agents=[create_translator_agent()],
            tasks=[create_translation_task(paper)],
            verbose=True,
            share_crew=False
        )
        translation_result = translation_crew.kickoff()
        translated_content = translation_result.raw.strip()
        
        # æ­¥éª¤2: è¯„å®¡
        print("  [æ­¥éª¤2/2] ä¸“ä¸šè¯„å®¡å’Œè¯„åˆ†ä¸­...")
        review_crew = Crew(
            agents=[create_reviewer_agent()],
            tasks=[create_review_task(paper, translated_content)],
            verbose=True,
            share_crew=False
        )
        review_result = review_crew.kickoff()
        review_text = review_result.raw.strip()
        
        # æå–è¯„åˆ†
        score_data = extract_score_from_review(review_text)
        
        return {
            'translated_content': translated_content,
            'review': review_text,
            'score': score_data.get('æ€»åˆ†', 0.0),
            'score_details': score_data,
            'is_high_value': score_data.get('æ€»åˆ†', 0.0) > 4.0
        }
    except Exception as e:
        logging.error(f"å¤„ç†è®ºæ–‡æ—¶å‡ºé”™: {str(e)}")
        return {
            'translated_content': f"ç¿»è¯‘å¤±è´¥: {str(e)}",
            'review': f"è¯„å®¡å¤±è´¥: {str(e)}",
            'score': 0.0,
            'score_details': {},
            'is_high_value': False
        }


def extract_score_from_review(review_text):
    """ä»è¯„å®¡æ–‡æœ¬ä¸­æå–è¯„åˆ†ä¿¡æ¯"""
    import json
    import re
    
    score_data = {
        'åˆ›æ–°æ€§': 0.0,
        'æŠ€æœ¯æ·±åº¦': 0.0,
        'ç›¸å…³æ€§': 0.0,
        'å®ç”¨æ€§': 0.0,
        'ç ”ç©¶è´¨é‡': 0.0,
        'æ€»åˆ†': 0.0,
        'è¯„åˆ†ç†ç”±': ''
    }
    
    # æ–¹æ³•1: å°è¯•æå–å®Œæ•´çš„JSONå¯¹è±¡ï¼ˆæ”¯æŒå¤šè¡Œå’ŒåµŒå¥—ï¼‰
    # æŸ¥æ‰¾JSONå¯¹è±¡çš„å¼€å§‹å’Œç»“æŸ
    json_start = review_text.find('{')
    json_end = review_text.rfind('}')
    if json_start != -1 and json_end != -1 and json_end > json_start:
        try:
            json_str = review_text[json_start:json_end+1]
            # å°è¯•è§£æJSON
            parsed = json.loads(json_str)
            # æ›´æ–°åˆ†æ•°æ•°æ®ï¼Œåªæ›´æ–°å­˜åœ¨çš„å­—æ®µ
            for key in score_data.keys():
                if key in parsed:
                    if isinstance(parsed[key], (int, float)):
                        score_data[key] = float(parsed[key])
                    elif isinstance(parsed[key], str) and key == 'è¯„åˆ†ç†ç”±':
                        score_data[key] = parsed[key]
            return score_data
        except (json.JSONDecodeError, ValueError):
            pass
    
    # æ–¹æ³•2: å°è¯•æå–JSONæ ¼å¼çš„è¯„åˆ†ï¼ˆæ›´å®½æ¾çš„åŒ¹é…ï¼‰
    json_patterns = [
        r'\{[^{}]*"æ€»åˆ†"[^{}]*\}',
        r'\{[^{}]*"åˆ›æ–°æ€§"[^{}]*"æŠ€æœ¯æ·±åº¦"[^{}]*\}',
    ]
    for pattern in json_patterns:
        json_match = re.search(pattern, review_text, re.DOTALL)
        if json_match:
            try:
                json_str = json_match.group(0)
                parsed = json.loads(json_str)
                for key in score_data.keys():
                    if key in parsed:
                        if isinstance(parsed[key], (int, float)):
                            score_data[key] = float(parsed[key])
                        elif isinstance(parsed[key], str) and key == 'è¯„åˆ†ç†ç”±':
                            score_data[key] = parsed[key]
                if score_data['æ€»åˆ†'] > 0:
                    return score_data
            except (json.JSONDecodeError, ValueError):
                continue
    
    # æ–¹æ³•3: å¦‚æœJSONæå–å¤±è´¥ï¼Œå°è¯•ä»æ–‡æœ¬ä¸­æå–æ•°å­—
    # æŸ¥æ‰¾æ€»åˆ†ï¼ˆæ”¯æŒå¤šç§æ ¼å¼ï¼‰
    total_score_patterns = [
        r'æ€»åˆ†[ï¼š:]\s*([0-9.]+)',
        r'ç»¼åˆè¯„åˆ†[ï¼š:]\s*([0-9.]+)',
        r'è¯„åˆ†[ï¼š:]\s*([0-9.]+)\s*[/ï¼]\s*5',
        r'([0-9.]+)\s*[/ï¼]\s*5\.0',
    ]
    for pattern in total_score_patterns:
        match = re.search(pattern, review_text)
        if match:
            try:
                score = float(match.group(1))
                if 0 <= score <= 5:
                    score_data['æ€»åˆ†'] = score
                    break
            except (ValueError, IndexError):
                continue
    
    # æŸ¥æ‰¾å„ä¸ªç»´åº¦çš„åˆ†æ•°
    dimensions = ['åˆ›æ–°æ€§', 'æŠ€æœ¯æ·±åº¦', 'ç›¸å…³æ€§', 'å®ç”¨æ€§', 'ç ”ç©¶è´¨é‡']
    for dim in dimensions:
        patterns = [
            rf'{dim}[ï¼š:]\s*([0-9.]+)',
            rf'"{dim}"[ï¼š:]\s*([0-9.]+)',
        ]
        for pattern in patterns:
            match = re.search(pattern, review_text)
            if match:
                try:
                    score = float(match.group(1))
                    if 0 <= score <= 1:
                        score_data[dim] = score
                        break
                except (ValueError, IndexError):
                    continue
    
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ€»åˆ†ï¼Œè®¡ç®—å„ç»´åº¦ä¹‹å’Œ
    if score_data['æ€»åˆ†'] == 0.0:
        total = sum([score_data[dim] for dim in dimensions])
        if total > 0:
            score_data['æ€»åˆ†'] = total
    
    return score_data


def generate_daily_report(relevant_papers):
    """ç”ŸæˆåŸå§‹æ—¥æŠ¥ï¼ˆMarkdown æ ¼å¼ï¼Œä¸ç†æƒ³æ ¼å¼ä¸€è‡´ï¼‰"""
    import json
    import re
    report = []
    
    # æŒ‰è¯„åˆ†åˆ†ç±»è®ºæ–‡
    high_value_papers = [p for p in relevant_papers if p.get('is_high_value', False)]
    other_papers = [p for p in relevant_papers if not p.get('is_high_value', False)]
    
    # æŒ‰è¯„åˆ†æ’åº
    high_value_papers.sort(key=lambda x: x.get('score', 0.0), reverse=True)
    other_papers.sort(key=lambda x: x.get('score', 0.0), reverse=True)
    
    def has_score_details_in_review(review_content):
        """æ£€æŸ¥reviewå†…å®¹ä¸­æ˜¯å¦å·²ç»åŒ…å«JSONä»£ç å—æ ¼å¼çš„è¯„åˆ†è¯¦æƒ…"""
        if not review_content:
            return False
        # æ£€æŸ¥æ˜¯å¦åŒ…å«```jsonä»£ç å—ï¼Œå¹¶ä¸”ä»£ç å—ä¸­åŒ…å«"æ€»åˆ†"å­—æ®µ
        # åŒ¹é…```jsonå¼€å§‹åˆ°```ç»“æŸä¹‹é—´çš„å†…å®¹ï¼ŒåŒ…å«"æ€»åˆ†"
        pattern = r'```json\s*.*?"æ€»åˆ†".*?```'
        return bool(re.search(pattern, review_content, re.DOTALL | re.IGNORECASE))
    
    # é«˜ä»·å€¼è®ºæ–‡ï¼ˆè¯„åˆ†>4.0ï¼Œéœ€è¦è¿›ä¸€æ­¥ç ”ç©¶ï¼‰
    if high_value_papers:
        report.append("## ğŸ”¥ é«˜ä»·å€¼è®ºæ–‡ï¼ˆè¯„åˆ†>4.0ï¼Œå»ºè®®ä¸‹è½½åŸæ–‡æ·±å…¥ç ”ç©¶ï¼‰")
        
        for i, paper in enumerate(high_value_papers, 1):
            report.append(f"### {i}. {paper['title']}")
            report.append("")
            
            # æ·»åŠ è¯„å®¡å†…å®¹
            review_content = paper.get('review', paper.get('summary', '')).strip()
            if review_content:
                report.append(review_content)
                report.append("")
            
            # æ£€æŸ¥reviewä¸­æ˜¯å¦å·²ç»åŒ…å«è¯„åˆ†è¯¦æƒ…ï¼Œå¦‚æœæ²¡æœ‰æ‰æ·»åŠ 
            if not has_score_details_in_review(review_content):
                score_details = paper.get('score_details', {})
                if score_details:
                    report.append("**è¯„åˆ†è¯¦æƒ…**ï¼š")
                    report.append("")
                    report.append("```json")
                    # æ ¼å¼åŒ–JSONï¼Œç¡®ä¿ç¾è§‚
                    json_str = json.dumps(score_details, ensure_ascii=False, indent=2)
                    report.append(json_str)
                    report.append("```")
                    report.append("")
            
            # æ·»åŠ è®ºæ–‡é“¾æ¥
            report.append(f"ğŸ”— [è®ºæ–‡é“¾æ¥]({paper['link']})")
            report.append("")
            
            # æ·»åŠ åˆ†éš”ç¬¦ï¼ˆæœ€åä¸€ä¸ªè®ºæ–‡åä¸æ·»åŠ ï¼‰
            if i < len(high_value_papers):
                report.append("---")
                report.append("")
    
    # å…¶ä»–ç›¸å…³è®ºæ–‡
    if other_papers:
        report.append("## ğŸ“– ç›¸å…³è®ºæ–‡")
        
        for i, paper in enumerate(other_papers, 1):
            report.append(f"### {i}. {paper['title']}")
            report.append("")
            
            # æ·»åŠ è¯„å®¡å†…å®¹
            review_content = paper.get('review', paper.get('summary', '')).strip()
            if review_content:
                report.append(review_content)
                report.append("")
            
            # æ£€æŸ¥reviewä¸­æ˜¯å¦å·²ç»åŒ…å«è¯„åˆ†è¯¦æƒ…ï¼Œå¦‚æœæ²¡æœ‰æ‰æ·»åŠ 
            if not has_score_details_in_review(review_content):
                # æ·»åŠ è¯„åˆ†
                report.append(f"**è¯„åˆ†ï¼š** {paper.get('score', 0.0):.2f}/5.0")
                report.append("")
                
                # æ·»åŠ è¯„åˆ†è¯¦æƒ…ï¼ˆJSONæ ¼å¼ï¼‰
                score_details = paper.get('score_details', {})
                if score_details:
                    report.append("**è¯„åˆ†è¯¦æƒ…**ï¼š")
                    report.append("")
                    report.append("```json")
                    json_str = json.dumps(score_details, ensure_ascii=False, indent=2)
                    report.append(json_str)
                    report.append("```")
                    report.append("")
            
            # æ·»åŠ è®ºæ–‡é“¾æ¥
            report.append(f"ğŸ”— [è®ºæ–‡é“¾æ¥]({paper['link']})")
            report.append("")
            
            # æ·»åŠ åˆ†éš”ç¬¦ï¼ˆæœ€åä¸€ä¸ªè®ºæ–‡åä¸æ·»åŠ ï¼‰
            if i < len(other_papers):
                report.append("---")
                report.append("")
    
    # ç»Ÿè®¡ä¿¡æ¯ï¼ˆä½¿ç”¨è¡¨æ ¼ï¼Œæ ¼å¼ä¸ç†æƒ³æ–‡ä»¶ä¸€è‡´ï¼‰
    report.append("## ğŸ“Š ç»Ÿè®¡ä¿¡æ¯")
    report.append("")
    report.append("| ç±»åˆ«            | æ•°é‡       |")
    report.append("| ------------- | -------- |")
    report.append(f"| é«˜ä»·å€¼è®ºæ–‡ï¼ˆè¯„åˆ†>4.0ï¼‰ | {len(high_value_papers)} ç¯‡     |")
    report.append(f"| å…¶ä»–ç›¸å…³è®ºæ–‡        | {len(other_papers)} ç¯‡      |")
    report.append(f"| **æ€»è®¡**        | **{len(relevant_papers)} ç¯‡** |")
    
    if high_value_papers:
        avg_score = sum(p.get('score', 0.0) for p in high_value_papers) / len(high_value_papers)
        report.append(f"| é«˜ä»·å€¼è®ºæ–‡å¹³å‡è¯„åˆ†     | {avg_score:.2f}/5.0 |")
    
    return "\n".join(report)


def export_high_value_papers_to_excel(relevant_papers, output_dir="reports"):
    """
    å°†é«˜ä»·å€¼è®ºæ–‡å¯¼å‡ºåˆ°Excelè¡¨æ ¼
    
    Args:
        relevant_papers: æ‰€æœ‰ç›¸å…³è®ºæ–‡åˆ—è¡¨
        output_dir: è¾“å‡ºç›®å½•
    """
    # ç­›é€‰é«˜ä»·å€¼è®ºæ–‡
    high_value_papers = [p for p in relevant_papers if p.get('is_high_value', False)]
    
    if not high_value_papers:
        print("\næ²¡æœ‰é«˜ä»·å€¼è®ºæ–‡éœ€è¦å¯¼å‡º")
        return
    
    # æŒ‰è¯„åˆ†æ’åº
    high_value_papers.sort(key=lambda x: x.get('score', 0.0), reverse=True)
    
    # å‡†å¤‡æ•°æ®
    excel_data = []
    
    for paper in high_value_papers:
        score_details = paper.get('score_details', {})
        
        # æ„å»ºæ•°æ®è¡Œ
        row = {
            'è®ºæ–‡æ ‡é¢˜': paper.get('title', ''),
            'è®ºæ–‡é“¾æ¥': paper.get('link', ''),
            'æ€»åˆ†': score_details.get('æ€»åˆ†', 0.0),
            'åˆ›æ–°æ€§': score_details.get('åˆ›æ–°æ€§', 0.0),
            'æŠ€æœ¯æ·±åº¦': score_details.get('æŠ€æœ¯æ·±åº¦', 0.0),
            'ç›¸å…³æ€§': score_details.get('ç›¸å…³æ€§', 0.0),
            'å®ç”¨æ€§': score_details.get('å®ç”¨æ€§', 0.0),
            'ç ”ç©¶è´¨é‡': score_details.get('ç ”ç©¶è´¨é‡', 0.0),
            'è¯„åˆ†ç†ç”±': score_details.get('è¯„åˆ†ç†ç”±', ''),
        }
        
        excel_data.append(row)
    
    # åˆ›å»ºDataFrame
    df = pd.DataFrame(excel_data)
    
    # ç”ŸæˆExcelæ–‡ä»¶å
    excel_filename = f"{output_dir}/é«˜ä»·å€¼è®ºæ–‡_{datetime.now().strftime('%Y%m%d')}.xlsx"
    
    # ä¿å­˜åˆ°Excel
    try:
        with pd.ExcelWriter(excel_filename, engine='openpyxl') as writer:
            df.to_excel(writer, sheet_name='é«˜ä»·å€¼è®ºæ–‡', index=False)
            
            # è·å–å·¥ä½œè¡¨å¯¹è±¡ä»¥è°ƒæ•´åˆ—å®½
            worksheet = writer.sheets['é«˜ä»·å€¼è®ºæ–‡']
            
            # è°ƒæ•´åˆ—å®½
            worksheet.column_dimensions['A'].width = 60  # è®ºæ–‡æ ‡é¢˜
            worksheet.column_dimensions['B'].width = 80  # è®ºæ–‡é“¾æ¥
            worksheet.column_dimensions['C'].width = 10  # æ€»åˆ†
            worksheet.column_dimensions['D'].width = 10  # åˆ›æ–°æ€§
            worksheet.column_dimensions['E'].width = 10  # æŠ€æœ¯æ·±åº¦
            worksheet.column_dimensions['F'].width = 10  # ç›¸å…³æ€§
            worksheet.column_dimensions['G'].width = 10  # å®ç”¨æ€§
            worksheet.column_dimensions['H'].width = 10  # ç ”ç©¶è´¨é‡
            worksheet.column_dimensions['I'].width = 50  # è¯„åˆ†ç†ç”±
            
            # è®¾ç½®æ ‡é¢˜è¡Œæ ·å¼ï¼ˆåŠ ç²—ï¼‰
            from openpyxl.styles import Font
            header_font = Font(bold=True)
            for cell in worksheet[1]:
                cell.font = header_font
        
        print(f"\nâœ“ é«˜ä»·å€¼è®ºæ–‡å·²å¯¼å‡ºåˆ°Excel: {excel_filename}")
        print(f"  - å…±å¯¼å‡º {len(high_value_papers)} ç¯‡é«˜ä»·å€¼è®ºæ–‡")
    except Exception as e:
        logging.error(f"å¯¼å‡ºExcelæ—¶å‡ºé”™: {str(e)}")
        print(f"\nâœ— å¯¼å‡ºExcelå¤±è´¥: {str(e)}")



def main():
    """ä¸»ç¨‹åº"""
    print("=" * 80)
    print("Paper Summarizer - å­¦æœ¯è®ºæ–‡è‡ªåŠ¨æ€»ç»“ç³»ç»Ÿ")
    print("=" * 80)
    print()
    
    # 1. è¿æ¥Gmail
    mail = connect_gmail()
    
    # 2. è·å–é‚®ä»¶ï¼ˆä»å‰START_DAYSå¤©åˆ°å‰END_DAYSå¤©ï¼‰
    email_ids = fetch_scholar_emails(mail, start_days=START_DAYS, end_days=END_DAYS)
    
    if not email_ids:
        print("\næ²¡æœ‰æ‰¾åˆ°æ–°çš„å­¦æœ¯æ¨é€é‚®ä»¶")
        mail.close()
        mail.logout()
        return
        
    # 3. å¤„ç†é‚®ä»¶
    all_papers = []
    
    try:
        for email_id in email_ids[:MAX_EMAILS]:
            print(f"\nå¤„ç†é‚®ä»¶ {email_id.decode()}...")
            
            try:
                status, msg_data = mail.fetch(email_id, "(RFC822)")
                
                if status != 'OK':
                    print(f"  è­¦å‘Š: æ— æ³•è·å–é‚®ä»¶å†…å®¹ (çŠ¶æ€: {status})")
                    continue
                
                for response_part in msg_data:
                    if isinstance(response_part, tuple):
                        msg = email.message_from_bytes(response_part[1])
                        
                        # éªŒè¯é‚®ä»¶æ—¥æœŸæ˜¯å¦åœ¨æŒ‡å®šèŒƒå›´å†…
                        if not is_email_in_date_range(msg, start_days=START_DAYS, end_days=END_DAYS):
                            email_date_str = msg.get('Date', 'æœªçŸ¥')
                            print(f"  è·³è¿‡: é‚®ä»¶æ—¥æœŸä¸åœ¨èŒƒå›´å†… ({email_date_str})")
                            continue
                        
                        # è·å–é‚®ä»¶æ­£æ–‡
                        if msg.is_multipart():
                            for part in msg.walk():
                                if part.get_content_type() == "text/html":
                                    body = part.get_payload(decode=True).decode()
                                    break
                        else:
                            body = msg.get_payload(decode=True).decode()
                        
                        # æå–è®ºæ–‡ä¿¡æ¯
                        papers = extract_paper_info(body)
                        all_papers.extend(papers)
                        print(f"  æå–åˆ° {len(papers)} ç¯‡è®ºæ–‡")
                        
            except (imaplib.IMAP4.error, ssl.SSLError, OSError) as e:
                print(f"  è­¦å‘Š: å¤„ç†é‚®ä»¶æ—¶å‡ºé”™: {str(e)}")
                # å°è¯•é‡æ–°è¿æ¥
                try:
                    mail.close()
                except:
                    pass
                try:
                    mail = connect_gmail()
                    mail.select("inbox")
                except Exception as reconnect_error:
                    print(f"  é”™è¯¯: é‡æ–°è¿æ¥å¤±è´¥: {str(reconnect_error)}")
                    break
                continue
            except Exception as e:
                print(f"  è­¦å‘Š: å¤„ç†é‚®ä»¶æ—¶å‡ºç°æœªçŸ¥é”™è¯¯: {str(e)}")
                continue
    finally:
        # ç¡®ä¿è¿æ¥è¢«æ­£ç¡®å…³é—­
        try:
            mail.close()
        except:
            pass
        try:
            mail.logout()
        except:
            pass
    
    print(f"\næ€»å…±æå–åˆ° {len(all_papers)} ç¯‡è®ºæ–‡")
    
    # 4. ç­›é€‰ç›¸å…³è®ºæ–‡
    print("\næ­£åœ¨åˆ†æè®ºæ–‡ç›¸å…³æ€§...")
    relevant_papers = []
    
    for paper in all_papers:
        relevance_score, is_high_priority = check_relevance(paper)
        
        if relevance_score > 0:  # è‡³å°‘åŒ¹é…ä¸€ä¸ªå…³é”®è¯
            paper['relevance_score'] = relevance_score
            paper['is_high_priority'] = is_high_priority
            relevant_papers.append(paper)
    
    print(f"âœ“ æ‰¾åˆ° {len(relevant_papers)} ç¯‡ç›¸å…³è®ºæ–‡")
    
    if not relevant_papers:
        print("\næ²¡æœ‰æ‰¾åˆ°ç›¸å…³è®ºæ–‡")
        return
    
    # 5. ä½¿ç”¨ CrewAI å¤„ç†è®ºæ–‡ï¼šç¿»è¯‘ + è¯„å®¡
    print("\næ­£åœ¨ä½¿ç”¨AIå¤„ç†è®ºæ–‡ï¼ˆç¿»è¯‘ + è¯„å®¡ï¼‰...")
    for i, paper in enumerate(relevant_papers, 1):
        print(f"\nå¤„ç† {i}/{len(relevant_papers)}: {paper['title'][:50]}...")
        
        # ä½¿ç”¨ CrewAI æ¡†æ¶å¤„ç†
        result = process_paper_with_crewai(paper)
        
        # æ›´æ–°è®ºæ–‡ä¿¡æ¯
        paper['translated_content'] = result['translated_content']
        paper['review'] = result['review']
        paper['score'] = result['score']
        paper['score_details'] = result['score_details']
        paper['is_high_value'] = result['is_high_value']
        
        print(f"  âœ“ å®Œæˆ - è¯„åˆ†: {paper['score']:.2f}/5.0", end="")
        if paper['is_high_value']:
            print(" [é«˜ä»·å€¼è®ºæ–‡ â­]")
        else:
            print()
    
    # 6. ç”Ÿæˆæ—¥æŠ¥
    print("\nç”Ÿæˆæ—¥æŠ¥...")
    report = generate_daily_report(relevant_papers)
    
    # 7. ä¿å­˜æŠ¥å‘Šï¼ˆMarkdown æ ¼å¼ï¼Œå‘½åæ–¹å¼ä¸ç†æƒ³æ ¼å¼ä¸€è‡´ï¼‰
    output_dir = "reports"
    os.makedirs(output_dir, exist_ok=True)
    
    # æ–‡ä»¶å‘½åæ ¼å¼ï¼šRobotics_Academic_Daily_YYYYMMDD .mdï¼ˆæ³¨æ„æœ‰ç©ºæ ¼ï¼‰
    filename = f"{output_dir}/Robotics_Academic_Daily_{datetime.now().strftime('%Y%m%d')}.md"
    with open(filename, 'w', encoding='utf-8') as f:
        f.write(report)
    
    print(f"\nâœ“ æŠ¥å‘Šå·²ä¿å­˜åˆ°: {filename}")
    
    # 7.1. å¦‚æœé…ç½®äº†å¤‡ä»½è·¯å¾„ï¼ŒåŒæ—¶ä¿å­˜åˆ°å¤‡ä»½ç›®å½•
    if BACKUP_DIR:
        try:
            os.makedirs(BACKUP_DIR, exist_ok=True)
            backup_filename = os.path.join(BACKUP_DIR, f"Robotics_Academic_Daily_{datetime.now().strftime('%Y%m%d')}.md")
            with open(backup_filename, 'w', encoding='utf-8') as f:
                f.write(report)
            print(f"âœ“ æŠ¥å‘Šå·²å¦å­˜åˆ°: {backup_filename}")
        except Exception as e:
            logging.warning(f"ä¿å­˜åˆ°å¤‡ä»½ç›®å½•å¤±è´¥: {str(e)}")
            print(f"âš  è­¦å‘Š: æ— æ³•ä¿å­˜åˆ°å¤‡ä»½ç›®å½•: {str(e)}")
    
    # 8. å¯¼å‡ºé«˜ä»·å€¼è®ºæ–‡åˆ°Excel
    export_high_value_papers_to_excel(relevant_papers, output_dir)
    
    print("\n" + "=" * 80)
    print(report)


if __name__ == "__main__":
    main()