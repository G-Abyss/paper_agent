#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Paper Summarizer - è‡ªåŠ¨æ€»ç»“Googleå­¦æœ¯é‚®ä»¶æ¨é€
"""

import os
import imaplib
import email
from email.header import decode_header
import re
from datetime import datetime, timedelta
import yaml
from dotenv import load_dotenv
import ollama
from bs4 import BeautifulSoup
import time
import ssl
from crewai import Agent, Task, Crew, LLM
import logging

# ç¦ç”¨ CrewAI é¥æµ‹ï¼ˆå¯é€‰ï¼‰
os.environ['CREWAI_TELEMETRY_OPT_OUT'] = 'true'
os.environ['OTEL_SDK_DISABLED'] = 'true'

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# é…ç½®
# GMAIL_USER = os.getenv('GMAIL_USER')
# GMAIL_PASSWORD = os.getenv('GMAIL_PASSWORD')
QMAIL_USER = os.getenv('QMAIL_USER')
QMAIL_PASSWORD = os.getenv('QMAIL_PASSWORD')
OLLAMA_MODEL = os.getenv('OLLAMA_MODEL', 'qwen2.5:32b')
MAX_EMAILS = int(os.getenv('MAX_EMAILS', 20))
OLLAMA_BASE_URL = os.getenv('OLLAMA_BASE_URL', 'http://localhost:11434')

# è®¾ç½®ç¯å¢ƒå˜é‡ï¼ˆCrewAI é€šè¿‡ LiteLLM è¿æ¥ Ollama éœ€è¦è¿™äº›ï¼‰
os.environ['OLLAMA_API_BASE'] = OLLAMA_BASE_URL
if not os.getenv('OPENAI_API_KEY'):
    os.environ['OPENAI_API_KEY'] = 'ollama'  # å ä½ç¬¦ï¼Œå®é™…ä¸ä½¿ç”¨

# åˆå§‹åŒ– CrewAI LLM
# å…³é”®ï¼šæ¨¡å‹åç§°å¿…é¡»åŒ…å« "ollama/" å‰ç¼€
llm_model_name = f"ollama/{OLLAMA_MODEL}" if not OLLAMA_MODEL.startswith("ollama/") else OLLAMA_MODEL

logging.info(f"åˆå§‹åŒ– CrewAI LLM: model={llm_model_name}, base_url={OLLAMA_BASE_URL}")

llm = LLM(
    model=llm_model_name,
    base_url=OLLAMA_BASE_URL,
    api_key="ollama"  # Ollama ä¸éœ€è¦çœŸå®çš„ API key
)

# åŠ è½½å…³é”®è¯
with open('keywords.yaml', 'r', encoding='utf-8') as f:
    KEYWORDS = yaml.safe_load(f)

HIGH_PRIORITY_KEYWORDS = [kw.lower() for kw in KEYWORDS['high_priority']]
RELATED_KEYWORDS = [kw.lower() for kw in KEYWORDS['related']]


def connect_gmail(max_retries=3, retry_delay=5):
    """è¿æ¥Gmail IMAPæœåŠ¡å™¨ï¼Œå¸¦é‡è¯•æœºåˆ¶"""
    print("æ­£åœ¨è¿æ¥Gmail...")
    
    for attempt in range(max_retries):
        try:
            # åˆ›å»º SSL ä¸Šä¸‹æ–‡
            context = ssl.create_default_context()
            
            # ä½¿ç”¨è¶…æ—¶è®¾ç½®è¿æ¥
            # mail = imaplib.IMAP4_SSL("imap.gmail.com", port=993, ssl_context=context)
            # mail.sock.settimeout(30)  # è®¾ç½®30ç§’è¶…æ—¶
            
            # mail.login(GMAIL_USER, GMAIL_PASSWORD)
            # print("âœ“ Gmailè¿æ¥æˆåŠŸ")

            mail = imaplib.IMAP4_SSL("imap.qq.com", port=993, ssl_context=context)
            mail.sock.settimeout(30)  # è®¾ç½®30ç§’è¶…æ—¶
            
            mail.login(QMAIL_USER, QMAIL_PASSWORD)
            print("âœ“ QQmailè¿æ¥æˆåŠŸ")
            return mail
            
        except (imaplib.IMAP4.error, ssl.SSLError, OSError) as e:
            if attempt < max_retries - 1:
                print(f"è¿æ¥å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {str(e)}")
                print(f"ç­‰å¾… {retry_delay} ç§’åé‡è¯•...")
                time.sleep(retry_delay)
            else:
                print(f"âœ— Gmailè¿æ¥å¤±è´¥ï¼Œå·²é‡è¯• {max_retries} æ¬¡")
                raise Exception(f"æ— æ³•è¿æ¥åˆ°Gmail: {str(e)}")
    
    raise Exception("æ— æ³•è¿æ¥åˆ°Gmail")


def fetch_scholar_emails(mail, days=1):
    """è·å–Googleå­¦æœ¯æ¨é€é‚®ä»¶"""
    print(f"\næ­£åœ¨è·å–æœ€è¿‘{days}å¤©çš„Googleå­¦æœ¯æ¨é€...")
    
    # é€‰æ‹©æ”¶ä»¶ç®±
    mail.select("inbox")
    
    # è®¡ç®—æ—¥æœŸ
    since_date = (datetime.now() - timedelta(days=days)).strftime("%d-%b-%Y")
    
    # æœç´¢Googleå­¦æœ¯é‚®ä»¶ï¼Œä»¥è½¬å‘é‚®ä»¶å‘ä»¶äººä¸ºligen4073187@gmail.comï¼Œä¸”è½¬å‘é‚®ä»¶ä¸­çš„åŸå§‹å‘ä»¶äººåœ°å€ä¸ºscholaralerts-noreply@google.com
    search_criteria = f'(FROM "ligen4073187@gmail.com" SINCE {since_date}) AND (HEADER FROM "scholaralerts-noreply@google.com")'
    status, messages = mail.search(None, search_criteria)
    
    email_ids = messages[0].split()
    print(f"âœ“ æ‰¾åˆ° {len(email_ids)} å°é‚®ä»¶")
    
    return email_ids


def extract_paper_info(email_body):
    """ä»é‚®ä»¶ä¸­æå–è®ºæ–‡ä¿¡æ¯"""
    soup = BeautifulSoup(email_body, 'html.parser')
    
    papers = []
    
    # Googleå­¦æœ¯æ¨é€çš„ç»“æ„é€šå¸¸åŒ…å«å¤šç¯‡è®ºæ–‡
    # æŸ¥æ‰¾æ‰€æœ‰è®ºæ–‡æ ‡é¢˜å’Œé“¾æ¥
    for h3 in soup.find_all('h3'):
        title_link = h3.find('a')
        if title_link:
            title = title_link.get_text(strip=True)
            link = title_link.get('href', '')
            
            # æŸ¥æ‰¾ä½œè€…å’Œæ‘˜è¦ä¿¡æ¯
            parent = h3.find_parent()
            if parent:
                text_content = parent.get_text()
                
                paper = {
                    'title': title,
                    'link': link,
                    'snippet': text_content[:500]  # è·å–å‰500å­—ç¬¦ä½œä¸ºç‰‡æ®µ
                }
                papers.append(paper)
    
    return papers


def check_relevance(paper):
    """æ£€æŸ¥è®ºæ–‡ç›¸å…³æ€§"""
    text = (paper['title'] + ' ' + paper['snippet']).lower()
    
    # æ£€æŸ¥é«˜ä¼˜å…ˆçº§å…³é”®è¯
    high_priority_matches = sum(1 for kw in HIGH_PRIORITY_KEYWORDS if kw in text)
    
    # æ£€æŸ¥ç›¸å…³å…³é”®è¯
    related_matches = sum(1 for kw in RELATED_KEYWORDS if kw in text)
    
    # è®¡ç®—ç›¸å…³æ€§åˆ†æ•°
    relevance_score = high_priority_matches * 2 + related_matches
    
    return relevance_score, high_priority_matches > 0


def create_translator_agent():
    """åˆ›å»ºä¸“ä¸šç¿»è¯‘ Agent"""
    return Agent(
        role="ä¸“ä¸šç¿»è¯‘ä¸“å®¶",
        goal="å°†è‹±æ–‡è®ºæ–‡å†…å®¹å‡†ç¡®ã€ä¸“ä¸šåœ°ç¿»è¯‘æˆä¸­æ–‡ï¼Œç¡®ä¿ä¸“ä¸šæœ¯è¯­çš„å‡†ç¡®æ€§å’ŒæŠ€æœ¯è¡¨è¾¾çš„æ¸…æ™°æ€§",
        backstory="ä½ æ˜¯ä¸€ä½åœ¨æœºå™¨äººå­¦ã€æ§åˆ¶ç†è®ºã€é¥æ“ä½œã€æœºå™¨äººåŠ¨åŠ›å­¦å’ŒåŠ›æ§é¢†åŸŸæ‹¥æœ‰æ·±åšä¸“ä¸šèƒŒæ™¯çš„ç¿»è¯‘ä¸“å®¶ã€‚ä½ æ“…é•¿å°†è‹±æ–‡å­¦æœ¯è®ºæ–‡ç¿»è¯‘æˆä¸­æ–‡ï¼Œèƒ½å¤Ÿå‡†ç¡®å¤„ç†ä¸“ä¸šæœ¯è¯­ï¼Œä¿æŒæŠ€æœ¯æè¿°çš„å®Œæ•´æ€§å’Œé€»è¾‘ç»“æ„çš„æ¸…æ™°æ€§ã€‚",
        allow_delegation=False,
        verbose=True,
        llm=llm,
        max_iter=3,
        max_execution_time=300
    )


def create_reviewer_agent():
    """åˆ›å»ºä¸“ä¸šè¯„å®¡ Agent"""
    return Agent(
        role="ä¸“ä¸šè¯„å®¡ä¸“å®¶",
        goal="å¯¹è®ºæ–‡è¿›è¡Œä¸“ä¸šè¯„å®¡ï¼Œç”Ÿæˆç»“æ„åŒ–æ€»ç»“å¹¶ç»™å‡ºç®€æ´çš„5åˆ†åˆ¶è¯„åˆ†ï¼ˆåªè¾“å‡ºä¸€æ¬¡ï¼‰",
        backstory="ä½ æ˜¯ä¸€ä½åœ¨æœºå™¨äººå­¦ã€æ§åˆ¶ç†è®ºã€é¥æ“ä½œã€æœºå™¨äººåŠ¨åŠ›å­¦å’ŒåŠ›æ§é¢†åŸŸæ‹¥æœ‰ä¸°å¯Œç ”ç©¶ç»éªŒçš„è¯„å®¡ä¸“å®¶ã€‚ä½ èƒ½å¤Ÿä»åˆ›æ–°æ€§ã€æŠ€æœ¯æ·±åº¦ã€ç›¸å…³æ€§ã€å®ç”¨æ€§å’Œç ”ç©¶è´¨é‡ç­‰å¤šä¸ªç»´åº¦å¯¹è®ºæ–‡è¿›è¡Œå®¢è§‚ã€ä¸“ä¸šçš„è¯„ä»·ã€‚ä½ æ€»æ˜¯ç®€æ´æ˜äº†åœ°è¾“å‡ºç»“æœï¼Œä¸ä¼šé‡å¤è¯´æ˜ã€‚",
        allow_delegation=False,
        verbose=True,
        llm=llm,
        max_iter=2,  # å‡å°‘è¿­ä»£æ¬¡æ•°ï¼Œé¿å…é‡å¤
        max_execution_time=300
    )


def create_translation_task(paper):
    """åˆ›å»ºç¿»è¯‘ä»»åŠ¡"""
    return Task(
        description=(
            f"è¯·å°†ä»¥ä¸‹è‹±æ–‡è®ºæ–‡ä¿¡æ¯å‡†ç¡®ã€ä¸“ä¸šåœ°ç¿»è¯‘æˆä¸­æ–‡ã€‚\n\n"
            f"è®ºæ–‡æ ‡é¢˜ï¼š\n{paper['title']}\n\n"
            f"è®ºæ–‡ç‰‡æ®µï¼ˆè‹±æ–‡åŸæ–‡ï¼‰ï¼š\n{paper['snippet']}\n\n"
            f"ç¿»è¯‘è¦æ±‚ï¼š\n"
            f"1. ä¿æŒä¸“ä¸šæœ¯è¯­çš„å‡†ç¡®æ€§ï¼Œä½¿ç”¨è¯¥é¢†åŸŸæ ‡å‡†çš„ä¸­æ–‡æœ¯è¯­\n"
            f"2. ç¡®ä¿æŠ€æœ¯æè¿°çš„å‡†ç¡®æ€§å’Œå®Œæ•´æ€§\n"
            f"3. ä¿æŒåŸæ–‡çš„é€»è¾‘ç»“æ„å’Œè¡¨è¾¾é£æ ¼\n"
            f"4. å¦‚æœé‡åˆ°ä¸ç¡®å®šçš„æœ¯è¯­ï¼Œè¯·æä¾›æœ€å¯èƒ½çš„ä¸“ä¸šç¿»è¯‘\n\n"
            f"è¯·ç›´æ¥è¾“å‡ºç¿»è¯‘ç»“æœï¼Œä¸éœ€è¦é¢å¤–è¯´æ˜ã€‚"
        ),
        agent=create_translator_agent(),
        expected_output="ç¿»è¯‘åçš„ä¸­æ–‡å†…å®¹ï¼Œä¿æŒåŸæ–‡çš„ç»“æ„å’Œé€»è¾‘"
    )


def create_review_task(paper, translated_content):
    """åˆ›å»ºè¯„å®¡ä»»åŠ¡"""
    return Task(
        description=(
            f"è¯·å¯¹ä»¥ä¸‹è®ºæ–‡è¿›è¡Œä¸“ä¸šè¯„å®¡ï¼Œç”Ÿæˆç»“æ„åŒ–æ€»ç»“å¹¶ç»™å‡ºè¯„åˆ†ã€‚\n\n"
            f"è®ºæ–‡æ ‡é¢˜ï¼ˆè‹±æ–‡ï¼‰ï¼š\n{paper['title']}\n\n"
            f"è®ºæ–‡å†…å®¹ï¼ˆå·²ç¿»è¯‘ä¸ºä¸­æ–‡ï¼‰ï¼š\n{translated_content}\n\n"
            f"è¯·æŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼ˆä¸è¦é‡å¤è¯´æ˜è¯„åˆ†è§„åˆ™ï¼Œç›´æ¥ç»™å‡ºç»“æœï¼‰ï¼š\n\n"
            f"**æ ¸å¿ƒè´¡çŒ®**ï¼šï¼ˆ1-2å¥è¯è¯´æ˜ä¸»è¦åˆ›æ–°ç‚¹å’Œè´¡çŒ®ï¼‰\n\n"
            f"**æŠ€æœ¯æ–¹æ³•**ï¼šï¼ˆç®€è¿°ä¸»è¦æŠ€æœ¯è·¯çº¿å’Œæ–¹æ³•ï¼‰\n\n"
            f"**ç›¸å…³æ€§åˆ†æ**ï¼šï¼ˆè¯¦ç»†è¯´æ˜ä¸é¥æ“ä½œ/æœºå™¨äººåŠ¨åŠ›å­¦/åŠ›æ§/æœºå™¨äººæ§åˆ¶çš„å…³ç³»ï¼‰\n\n"
            f"**æŠ€æœ¯ä»·å€¼**ï¼šï¼ˆè¯„ä¼°è¯¥è®ºæ–‡çš„æŠ€æœ¯ä»·å€¼å’Œæ½œåœ¨åº”ç”¨ï¼‰\n\n"
            f"**å€¼å¾—å…³æ³¨çš„åŸå› **ï¼šï¼ˆä¸ºä»€ä¹ˆè¿™ç¯‡è®ºæ–‡é‡è¦ï¼Œæœ‰å“ªäº›äº®ç‚¹ï¼‰\n\n"
            f"**è¯„åˆ†è¯¦æƒ…**ï¼ˆä»…è¾“å‡ºä¸€æ¬¡ï¼ŒJSONæ ¼å¼ï¼ŒåŒ…å«è¯„åˆ†ç†ç”±ï¼‰ï¼š\n"
            f'{{"åˆ›æ–°æ€§": 0.0-1.0, "æŠ€æœ¯æ·±åº¦": 0.0-1.0, "ç›¸å…³æ€§": 0.0-1.0, "å®ç”¨æ€§": 0.0-1.0, "ç ”ç©¶è´¨é‡": 0.0-1.0, "æ€»åˆ†": 0.0-5.0, "è¯„åˆ†ç†ç”±": "ç®€è¦è¯´æ˜è¯„åˆ†ä¾æ®"}}\n\n'
            f"é‡è¦ï¼šåªè¾“å‡ºä¸€æ¬¡è¯„åˆ†è¯¦æƒ…ï¼Œè¯„åˆ†ç†ç”±å¿…é¡»åŒ…å«åœ¨JSONä¸­ï¼Œä¸è¦é‡å¤è¯´æ˜è¯„åˆ†è§„åˆ™æˆ–å¤šæ¬¡è¾“å‡ºè¯„åˆ†ã€‚"
        ),
        agent=create_reviewer_agent(),
        expected_output=(
            "è¯„å®¡æŠ¥å‘ŠåŒ…å«ï¼šæ ¸å¿ƒè´¡çŒ®ã€æŠ€æœ¯æ–¹æ³•ã€ç›¸å…³æ€§åˆ†æã€æŠ€æœ¯ä»·å€¼ã€å€¼å¾—å…³æ³¨çš„åŸå› ï¼Œ"
            "ä»¥åŠä¸€ä¸ªJSONæ ¼å¼çš„è¯„åˆ†è¯¦æƒ…ï¼ˆåŒ…å«å„ç»´åº¦åˆ†æ•°ã€æ€»åˆ†å’Œè¯„åˆ†ç†ç”±ï¼Œä¸è¦é‡å¤è¾“å‡ºï¼‰ã€‚"
        )
    )


def process_paper_with_crewai(paper):
    """
    ä½¿ç”¨ CrewAI æ¡†æ¶å¤„ç†è®ºæ–‡ï¼šç¿»è¯‘ + è¯„å®¡
    è¿”å›å¤„ç†ç»“æœå­—å…¸
    """
    try:
        # æ­¥éª¤1: ç¿»è¯‘
        print("  [æ­¥éª¤1/2] ä¸“ä¸šç¿»è¯‘ä¸­...")
        translation_crew = Crew(
            agents=[create_translator_agent()],
            tasks=[create_translation_task(paper)],
            verbose=True,
            share_crew=False
        )
        translation_result = translation_crew.kickoff()
        translated_content = translation_result.raw.strip()
        
        # æ­¥éª¤2: è¯„å®¡
        print("  [æ­¥éª¤2/2] ä¸“ä¸šè¯„å®¡å’Œè¯„åˆ†ä¸­...")
        review_crew = Crew(
            agents=[create_reviewer_agent()],
            tasks=[create_review_task(paper, translated_content)],
            verbose=True,
            share_crew=False
        )
        review_result = review_crew.kickoff()
        review_text = review_result.raw.strip()
        
        # æå–è¯„åˆ†
        score_data = extract_score_from_review(review_text)
        
        return {
            'translated_content': translated_content,
            'review': review_text,
            'score': score_data.get('æ€»åˆ†', 0.0),
            'score_details': score_data,
            'is_high_value': score_data.get('æ€»åˆ†', 0.0) > 4.0
        }
    except Exception as e:
        logging.error(f"å¤„ç†è®ºæ–‡æ—¶å‡ºé”™: {str(e)}")
        return {
            'translated_content': f"ç¿»è¯‘å¤±è´¥: {str(e)}",
            'review': f"è¯„å®¡å¤±è´¥: {str(e)}",
            'score': 0.0,
            'score_details': {},
            'is_high_value': False
        }


def extract_score_from_review(review_text):
    """ä»è¯„å®¡æ–‡æœ¬ä¸­æå–è¯„åˆ†ä¿¡æ¯"""
    import json
    import re
    
    score_data = {
        'åˆ›æ–°æ€§': 0.0,
        'æŠ€æœ¯æ·±åº¦': 0.0,
        'ç›¸å…³æ€§': 0.0,
        'å®ç”¨æ€§': 0.0,
        'ç ”ç©¶è´¨é‡': 0.0,
        'æ€»åˆ†': 0.0,
        'è¯„åˆ†ç†ç”±': ''
    }
    
    # æ–¹æ³•1: å°è¯•æå–å®Œæ•´çš„JSONå¯¹è±¡ï¼ˆæ”¯æŒå¤šè¡Œå’ŒåµŒå¥—ï¼‰
    # æŸ¥æ‰¾JSONå¯¹è±¡çš„å¼€å§‹å’Œç»“æŸ
    json_start = review_text.find('{')
    json_end = review_text.rfind('}')
    if json_start != -1 and json_end != -1 and json_end > json_start:
        try:
            json_str = review_text[json_start:json_end+1]
            # å°è¯•è§£æJSON
            parsed = json.loads(json_str)
            # æ›´æ–°åˆ†æ•°æ•°æ®ï¼Œåªæ›´æ–°å­˜åœ¨çš„å­—æ®µ
            for key in score_data.keys():
                if key in parsed:
                    if isinstance(parsed[key], (int, float)):
                        score_data[key] = float(parsed[key])
                    elif isinstance(parsed[key], str) and key == 'è¯„åˆ†ç†ç”±':
                        score_data[key] = parsed[key]
            return score_data
        except (json.JSONDecodeError, ValueError):
            pass
    
    # æ–¹æ³•2: å°è¯•æå–JSONæ ¼å¼çš„è¯„åˆ†ï¼ˆæ›´å®½æ¾çš„åŒ¹é…ï¼‰
    json_patterns = [
        r'\{[^{}]*"æ€»åˆ†"[^{}]*\}',
        r'\{[^{}]*"åˆ›æ–°æ€§"[^{}]*"æŠ€æœ¯æ·±åº¦"[^{}]*\}',
    ]
    for pattern in json_patterns:
        json_match = re.search(pattern, review_text, re.DOTALL)
        if json_match:
            try:
                json_str = json_match.group(0)
                parsed = json.loads(json_str)
                for key in score_data.keys():
                    if key in parsed:
                        if isinstance(parsed[key], (int, float)):
                            score_data[key] = float(parsed[key])
                        elif isinstance(parsed[key], str) and key == 'è¯„åˆ†ç†ç”±':
                            score_data[key] = parsed[key]
                if score_data['æ€»åˆ†'] > 0:
                    return score_data
            except (json.JSONDecodeError, ValueError):
                continue
    
    # æ–¹æ³•3: å¦‚æœJSONæå–å¤±è´¥ï¼Œå°è¯•ä»æ–‡æœ¬ä¸­æå–æ•°å­—
    # æŸ¥æ‰¾æ€»åˆ†ï¼ˆæ”¯æŒå¤šç§æ ¼å¼ï¼‰
    total_score_patterns = [
        r'æ€»åˆ†[ï¼š:]\s*([0-9.]+)',
        r'ç»¼åˆè¯„åˆ†[ï¼š:]\s*([0-9.]+)',
        r'è¯„åˆ†[ï¼š:]\s*([0-9.]+)\s*[/ï¼]\s*5',
        r'([0-9.]+)\s*[/ï¼]\s*5\.0',
    ]
    for pattern in total_score_patterns:
        match = re.search(pattern, review_text)
        if match:
            try:
                score = float(match.group(1))
                if 0 <= score <= 5:
                    score_data['æ€»åˆ†'] = score
                    break
            except (ValueError, IndexError):
                continue
    
    # æŸ¥æ‰¾å„ä¸ªç»´åº¦çš„åˆ†æ•°
    dimensions = ['åˆ›æ–°æ€§', 'æŠ€æœ¯æ·±åº¦', 'ç›¸å…³æ€§', 'å®ç”¨æ€§', 'ç ”ç©¶è´¨é‡']
    for dim in dimensions:
        patterns = [
            rf'{dim}[ï¼š:]\s*([0-9.]+)',
            rf'"{dim}"[ï¼š:]\s*([0-9.]+)',
        ]
        for pattern in patterns:
            match = re.search(pattern, review_text)
            if match:
                try:
                    score = float(match.group(1))
                    if 0 <= score <= 1:
                        score_data[dim] = score
                        break
                except (ValueError, IndexError):
                    continue
    
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ€»åˆ†ï¼Œè®¡ç®—å„ç»´åº¦ä¹‹å’Œ
    if score_data['æ€»åˆ†'] == 0.0:
        total = sum([score_data[dim] for dim in dimensions])
        if total > 0:
            score_data['æ€»åˆ†'] = total
    
    return score_data


def generate_daily_report(relevant_papers):
    """ç”ŸæˆåŸå§‹æ—¥æŠ¥ï¼ˆMarkdown æ ¼å¼ï¼Œç®€æ´ç‰ˆï¼‰"""
    report = []
    
    # æ ‡é¢˜
    report.append(f"# ğŸ“š æœºå™¨äººå­¦æœ¯è®ºæ–‡æ—¥æŠ¥")
    report.append(f"**æ—¥æœŸï¼š** {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥')}")
    report.append("")
    
    # æŒ‰è¯„åˆ†åˆ†ç±»è®ºæ–‡
    high_value_papers = [p for p in relevant_papers if p.get('is_high_value', False)]
    other_papers = [p for p in relevant_papers if not p.get('is_high_value', False)]
    
    # æŒ‰è¯„åˆ†æ’åº
    high_value_papers.sort(key=lambda x: x.get('score', 0.0), reverse=True)
    other_papers.sort(key=lambda x: x.get('score', 0.0), reverse=True)
    
    # é«˜ä»·å€¼è®ºæ–‡ï¼ˆè¯„åˆ†>4.0ï¼Œéœ€è¦è¿›ä¸€æ­¥ç ”ç©¶ï¼‰
    if high_value_papers:
        report.append("## ğŸ”¥ é«˜ä»·å€¼è®ºæ–‡ï¼ˆè¯„åˆ†>4.0ï¼Œå»ºè®®ä¸‹è½½åŸæ–‡æ·±å…¥ç ”ç©¶ï¼‰")
        report.append("")
        
        for i, paper in enumerate(high_value_papers, 1):
            report.append(f"### {i}. {paper['title']}")
            report.append("")
            report.append(paper.get('review', paper.get('summary', '')))
            report.append("")
            
            # # ç®€æ´çš„è¯„åˆ†å±•ç¤ºï¼ˆåˆå¹¶è¯„åˆ†è¯¦æƒ…å’Œè¯„åˆ†ç†ç”±ï¼‰
            # score = paper.get('score', 0.0)
            # score_details = paper.get('score_details', {})
            
            # # ä½¿ç”¨è¡¨æ ¼å±•ç¤ºè¯„åˆ†ï¼ˆæ›´ç›´è§‚ï¼‰ï¼Œè¯„åˆ†ç†ç”±åˆå¹¶åˆ°è¡¨æ ¼æœ€å
            # if score_details:
            #     report.append("**è¯„åˆ†è¯¦æƒ…ï¼š**")
            #     report.append("")
            #     report.append("| ç»´åº¦ | åˆ†æ•° |")
            #     report.append("|------|------|")
            #     for dim in ['åˆ›æ–°æ€§', 'æŠ€æœ¯æ·±åº¦', 'ç›¸å…³æ€§', 'å®ç”¨æ€§', 'ç ”ç©¶è´¨é‡']:
            #         if dim in score_details:
            #             dim_score = score_details[dim]
            #             # ç”¨æ˜Ÿæ˜Ÿè¡¨ç¤ºåˆ†æ•°
            #             stars = "â­" * int(dim_score * 5)
            #             report.append(f"| {dim} | {dim_score:.2f}/1.0 {stars} |")
            #     report.append(f"| **æ€»åˆ†** | **{score:.2f}/5.0** |")
            #     # å°†è¯„åˆ†ç†ç”±åˆå¹¶åˆ°è¡¨æ ¼æœ€å
            #     if 'è¯„åˆ†ç†ç”±' in score_details and score_details['è¯„åˆ†ç†ç”±']:
            #         report.append(f"| **è¯„åˆ†ç†ç”±** | {score_details['è¯„åˆ†ç†ç”±']} |")
            #     report.append("")
            
            report.append(f"ğŸ”— [è®ºæ–‡é“¾æ¥]({paper['link']})")
            report.append("")
            report.append("---")
            report.append("")
    
    # å…¶ä»–ç›¸å…³è®ºæ–‡
    if other_papers:
        report.append("## ğŸ“– ç›¸å…³è®ºæ–‡")
        report.append("")
        
        for i, paper in enumerate(other_papers, 1):
            report.append(f"### {i}. {paper['title']}")
            report.append("")
            report.append(paper.get('review', paper.get('summary', '')))
            report.append("")
            report.append(f"**è¯„åˆ†ï¼š** {paper.get('score', 0.0):.2f}/5.0")
            report.append(f"ğŸ”— [è®ºæ–‡é“¾æ¥]({paper['link']})")
            report.append("")
            report.append("---")
            report.append("")
    
    # ç»Ÿè®¡ä¿¡æ¯ï¼ˆä½¿ç”¨è¡¨æ ¼ï¼‰
    report.append("## ğŸ“Š ç»Ÿè®¡ä¿¡æ¯")
    report.append("")
    report.append("| ç±»åˆ« | æ•°é‡ |")
    report.append("|------|------|")
    report.append(f"| é«˜ä»·å€¼è®ºæ–‡ï¼ˆè¯„åˆ†>4.0ï¼‰ | {len(high_value_papers)} ç¯‡ |")
    report.append(f"| å…¶ä»–ç›¸å…³è®ºæ–‡ | {len(other_papers)} ç¯‡ |")
    report.append(f"| **æ€»è®¡** | **{len(relevant_papers)} ç¯‡** |")
    
    if high_value_papers:
        avg_score = sum(p.get('score', 0.0) for p in high_value_papers) / len(high_value_papers)
        report.append(f"| é«˜ä»·å€¼è®ºæ–‡å¹³å‡è¯„åˆ† | {avg_score:.2f}/5.0 |")
    
    report.append("")
    
    return "\n".join(report)




def main():
    """ä¸»ç¨‹åº"""
    print("=" * 80)
    print("Paper Summarizer - å­¦æœ¯è®ºæ–‡è‡ªåŠ¨æ€»ç»“ç³»ç»Ÿ")
    print("=" * 80)
    print()
    
    # 1. è¿æ¥Gmail
    mail = connect_gmail()
    
    # 2. è·å–é‚®ä»¶
    email_ids = fetch_scholar_emails(mail, days=1)
    
    if not email_ids:
        print("\næ²¡æœ‰æ‰¾åˆ°æ–°çš„å­¦æœ¯æ¨é€é‚®ä»¶")
        mail.close()
        mail.logout()
        return
    
    # 3. å¤„ç†é‚®ä»¶
    all_papers = []
    
    try:
        for email_id in email_ids[:MAX_EMAILS]:
            print(f"\nå¤„ç†é‚®ä»¶ {email_id.decode()}...")
            
            try:
                status, msg_data = mail.fetch(email_id, "(RFC822)")
                
                if status != 'OK':
                    print(f"  è­¦å‘Š: æ— æ³•è·å–é‚®ä»¶å†…å®¹ (çŠ¶æ€: {status})")
                    continue
                
                for response_part in msg_data:
                    if isinstance(response_part, tuple):
                        msg = email.message_from_bytes(response_part[1])
                        
                        # è·å–é‚®ä»¶æ­£æ–‡
                        if msg.is_multipart():
                            for part in msg.walk():
                                if part.get_content_type() == "text/html":
                                    body = part.get_payload(decode=True).decode()
                                    break
                        else:
                            body = msg.get_payload(decode=True).decode()
                        
                        # æå–è®ºæ–‡ä¿¡æ¯
                        papers = extract_paper_info(body)
                        all_papers.extend(papers)
                        print(f"  æå–åˆ° {len(papers)} ç¯‡è®ºæ–‡")
                        
            except (imaplib.IMAP4.error, ssl.SSLError, OSError) as e:
                print(f"  è­¦å‘Š: å¤„ç†é‚®ä»¶æ—¶å‡ºé”™: {str(e)}")
                # å°è¯•é‡æ–°è¿æ¥
                try:
                    mail.close()
                except:
                    pass
                try:
                    mail = connect_gmail()
                    mail.select("inbox")
                except Exception as reconnect_error:
                    print(f"  é”™è¯¯: é‡æ–°è¿æ¥å¤±è´¥: {str(reconnect_error)}")
                    break
                continue
            except Exception as e:
                print(f"  è­¦å‘Š: å¤„ç†é‚®ä»¶æ—¶å‡ºç°æœªçŸ¥é”™è¯¯: {str(e)}")
                continue
    finally:
        # ç¡®ä¿è¿æ¥è¢«æ­£ç¡®å…³é—­
        try:
            mail.close()
        except:
            pass
        try:
            mail.logout()
        except:
            pass
    
    print(f"\næ€»å…±æå–åˆ° {len(all_papers)} ç¯‡è®ºæ–‡")
    
    # 4. ç­›é€‰ç›¸å…³è®ºæ–‡
    print("\næ­£åœ¨åˆ†æè®ºæ–‡ç›¸å…³æ€§...")
    relevant_papers = []
    
    for paper in all_papers:
        relevance_score, is_high_priority = check_relevance(paper)
        
        if relevance_score > 0:  # è‡³å°‘åŒ¹é…ä¸€ä¸ªå…³é”®è¯
            paper['relevance_score'] = relevance_score
            paper['is_high_priority'] = is_high_priority
            relevant_papers.append(paper)
    
    print(f"âœ“ æ‰¾åˆ° {len(relevant_papers)} ç¯‡ç›¸å…³è®ºæ–‡")
    
    if not relevant_papers:
        print("\næ²¡æœ‰æ‰¾åˆ°ç›¸å…³è®ºæ–‡")
        return
    
    # 5. ä½¿ç”¨ CrewAI å¤„ç†è®ºæ–‡ï¼šç¿»è¯‘ + è¯„å®¡
    print("\næ­£åœ¨ä½¿ç”¨AIå¤„ç†è®ºæ–‡ï¼ˆç¿»è¯‘ + è¯„å®¡ï¼‰...")
    for i, paper in enumerate(relevant_papers, 1):
        print(f"\nå¤„ç† {i}/{len(relevant_papers)}: {paper['title'][:50]}...")
        
        # ä½¿ç”¨ CrewAI æ¡†æ¶å¤„ç†
        result = process_paper_with_crewai(paper)
        
        # æ›´æ–°è®ºæ–‡ä¿¡æ¯
        paper['translated_content'] = result['translated_content']
        paper['review'] = result['review']
        paper['score'] = result['score']
        paper['score_details'] = result['score_details']
        paper['is_high_value'] = result['is_high_value']
        
        print(f"  âœ“ å®Œæˆ - è¯„åˆ†: {paper['score']:.2f}/5.0", end="")
        if paper['is_high_value']:
            print(" [é«˜ä»·å€¼è®ºæ–‡ â­]")
        else:
            print()
    
    # 6. ç”Ÿæˆæ—¥æŠ¥
    print("\nç”Ÿæˆæ—¥æŠ¥...")
    report = generate_daily_report(relevant_papers)
    
    # 7. ä¿å­˜æŠ¥å‘Šï¼ˆMarkdown æ ¼å¼ï¼‰
    output_dir = "reports"
    os.makedirs(output_dir, exist_ok=True)
    
    filename = f"{output_dir}/paper_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
    with open(filename, 'w', encoding='utf-8') as f:
        f.write(report)
    
    print(f"\nâœ“ æŠ¥å‘Šå·²ä¿å­˜åˆ°: {filename}")
    print("\n" + "=" * 80)
    print(report)


if __name__ == "__main__":
    main()